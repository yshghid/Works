{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH21-AGENT\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "logging.langsmith(\"CH21-AGENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools \n",
    "\n",
    "- 도구(Tool) : Agent, Chain 또는 LLM이 외부와 상호작용 하기 위한 인터페이스 \n",
    "- LangChain에서 기본적으로 제공하는 도구, 또는 Custom Tool 구성하여 적용 \n",
    "- LangChain 도구 리스트 : https://python.langchain.com/v0.1/docs/integrations/tools/\n",
    "\n",
    "\n",
    "## Built-in Tools \n",
    "- LangChain에서 제공하는 사전 정의된 took & toolkit \n",
    "    - tool : 단일 도구 \n",
    "    - toolkit : 여러 도구를 묶어서 하나의 도구로 사용할 수 있음 \n",
    "    - https://python.langchain.com/docs/integrations/tools/\n",
    "\n",
    "### Python REPL Tools \n",
    "- 파이썬 코드를 REPL 환경에서 실행하기 위한 클래스 제공 \n",
    "    - REPL : Read-Eval-Print Loop, 파이썬을 바로 실행해 볼 수 있는 대화형 환경 \n",
    "    - https://python.langchain.com/docs/integrations/tools/python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_experimental.tools import PythonREPLTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonREPLTool imported successfully\n"
     ]
    }
   ],
   "source": [
    "# PythonREPLTool import 오류 해결을 위한 대안 방법들\n",
    "try:\n",
    "    from langchain_experimental.tools import PythonREPLTool\n",
    "    print(\"PythonREPLTool imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"PythonREPLTool import failed: {e}\")\n",
    "    print(\"Trying alternative import methods...\")\n",
    "    \n",
    "    # 대안 1: langchain_community에서 import 시도\n",
    "    try:\n",
    "        from langchain_community.tools import PythonREPLTool\n",
    "        print(\"PythonREPLTool imported from langchain_community\")\n",
    "    except Exception as e2:\n",
    "        print(f\"langchain_community import failed: {e2}\")\n",
    "        \n",
    "        # 대안 2: 직접 구현\n",
    "        print(\"Creating custom PythonREPLTool...\")\n",
    "        from langchain_core.tools import BaseTool\n",
    "        from typing import Any, Dict\n",
    "        import subprocess\n",
    "        import sys\n",
    "        \n",
    "        class PythonREPLTool(BaseTool):\n",
    "            name: str = \"python_repl\"\n",
    "            description: str = \"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\"\n",
    "            \n",
    "            def _run(self, query: str) -> str:\n",
    "                \"\"\"Use the tool.\"\"\"\n",
    "                try:\n",
    "                    # Create a subprocess to run Python code\n",
    "                    result = subprocess.run(\n",
    "                        [sys.executable, \"-c\", query],\n",
    "                        capture_output=True,\n",
    "                        text=True,\n",
    "                        timeout=30\n",
    "                    )\n",
    "                    if result.returncode != 0:\n",
    "                        return f\"Error: {result.stderr}\"\n",
    "                    return result.stdout\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    return \"Error: Code execution timed out\"\n",
    "                except Exception as e:\n",
    "                    return f\"Error: {str(e)}\"\n",
    "        \n",
    "        print(\"Custom PythonREPLTool created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    }
   ],
   "source": [
    "python_tool = PythonREPLTool()\n",
    "\n",
    "print(python_tool.invoke(\n",
    "    \"print (100 + 500)\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM 적용 \n",
    "\n",
    "기본 흐름 \n",
    "1. LLM 모델에게 특정 작업을 수행하는 파이썬 코드 작성 요청 \n",
    "2. 작성된 코드를 실행하여 결과 획득 \n",
    "3. 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_and_execute(code, debug=True):\n",
    "    if debug:\n",
    "        print(\"CODE: \")\n",
    "        print(code)\n",
    "    \n",
    "    return python_tool.invoke(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are Raymond Hetting, an expert python programmer, well versed in meta-programming and elegant, concise and short but well documented code. You follow the PEP8 style guide. \"\n",
    "            \"Return only the code, no intro, no explanation, no chatty, no markdown, no code block, no nothing. Just the code.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "chain = prompt | llm | StrOutputParser() | RunnableLambda(print_and_execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE: \n",
      "\n",
      "\n",
      "import random\n",
      "\n",
      "# Generate 6 random numbers between 1 and 45\n",
      "lotto_numbers = random.sample(range(1, 46), 6)\n",
      "\n",
      "# Sort the numbers in ascending order\n",
      "lotto_numbers.sort()\n",
      "\n",
      "# Print the numbers\n",
      "print(lotto_numbers)\n",
      "[2, 5, 25, 29, 32, 44]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"로또 번호 생성기를 출력하는 코드를 작성하세요.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색API 도구\n",
    "\n",
    "- Tavily API : 검색 기능을 구현해 놓은 도구 \n",
    "- [Ref] https://python.langchain.com/docs/integrations/tools/tavily_search/\n",
    "\n",
    "- 사전준비 : \n",
    "    - API 키 발급 : https://app.tavily.com/\n",
    "    - 발급받은 키를 `.env` 파일에 반영 (TAVILY_API_KEY=tv...)\n",
    "\n",
    "- 주요 클래스 : \n",
    "    - `TavilyAnswer`\n",
    "    - `TavilySerachResults`\n",
    "        - API 통해 검색하고 JSON 형식을 결과를 반환 \n",
    "        - 주요 매개 변수 \n",
    "            - `max_results` (int): 반환할 최대 검색 결과 수 (기본값: 5)\n",
    "            - `search_depth` (str): 검색 깊이 (\"basic\" 또는 \"advanced\")\n",
    "            - `include_domains` (List[str]): 검색 결과에 포함할 도메인 목록\n",
    "            - `exclude_domains` (List[str]): 검색 결과에서 제외할 도메인 목록\n",
    "            - `include_answer` (bool): 원본 쿼리에 대한 짧은 답변 포함 여부\n",
    "            - `include_raw_content` (bool): 각 사이트의 정제된 HTML 콘텐츠 포함 여부\n",
    "            - `include_images` (bool): 쿼리 관련 이미지 목록 포함 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yshmbid/Documents/home/github/Agent/agent-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3699: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5_/cr0lgx655m7fqg_j4q733qbm0000gn/T/ipykernel_27640/1072773976.py:2: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tool = TavilySearchResults(\n"
     ]
    }
   ],
   "source": [
    "# 도구 생성 \n",
    "tool = TavilySearchResults(\n",
    "    max_results=6,\n",
    "    include_answer=True,\n",
    "    include_raw_content=True,\n",
    "    # include_images=True,\n",
    "    # search_depth=\"advanced\" # or \"basic\"\n",
    "    include_domains=[\"github.io\",\"wikidocs.net\"],\n",
    "    # exclude_domains=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'LangChain, chains and agents, a great piece of engineering ...',\n",
       "  'url': 'https://datasciencebyexample.github.io/2023/05/27/understanding-langchain-chains-and-agents/',\n",
       "  'content': '| ``` # Set up a prompt templateclass CustomPromptTemplate(BaseChatPromptTemplate):     # The template to use    template: str     # The list of tools available    tools: List[Tool]         def format_messages(self, **kwargs) -> str:        # Get the intermediate steps (AgentAction, Observation tuples)         # Format them in a particular way        intermediate_steps = kwargs.pop(\"intermediate_steps\")        thoughts = \"\"        for action, observation in intermediate_steps:            thoughts += action.log            thoughts += f\"\\\\nObservation: {observation}\\\\nThought: \"         # Set the agent_scratchpad variable to that value        kwargs[\"agent_scratchpad\"] = thoughts         # Create a tools variable from the list of tools provided        kwargs[\"tools\"] = \"\\\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])         # Create a list of tool names for the tools provided        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])        formatted = self.template.format(**kwargs)        return [HumanMessage(content=formatted)] ``` |',\n",
       "  'score': 0.98526,\n",
       "  'raw_content': '---\\n\\nAccording to the official site, LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model, but will also be:\\n\\n1. Data-aware: connect a language model to other sources of data\\n2. Agentic: allow a language model to interact with its environment\\n\\nThe LangChain framework is designed around these principles, with the large language model (llm) as the engine where we must assume the llm works as it expects.\\n\\nTwo most important concepts in Langchain are `chains` and `agents`.\\n\\n## Chains\\n\\nUsing an LLM in isolation is fine for some simple applications, but many more complex ones require chaining LLMs - either with each other or with other experts. LangChain provides a standard interface for Chains, as well as some common implementations of chains for ease of use.\\n\\n### Why do we need chains?\\n\\nChains allow us to combine multiple components together to create a single, coherent application. For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM. We can build more complex chains by combining multiple chains together, or by combining chains with other components.\\n\\n### Quick start: Using LLMChain\\n\\nThe LLMChain is a simple chain that takes in a prompt template, formats it with the user input and returns the response from an LLM.\\n\\nTo use the LLMChain, first create a prompt template.\\n\\n|  |\\n| --- |\\n| ``` from langchain.prompts import PromptTemplatefrom langchain.llms import OpenAI llm = OpenAI(temperature=0.9)prompt = PromptTemplate(    input_variables=[\"product\"],    template=\"What is a good name for a company that makes {product}?\",) ``` |\\n\\nWe can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM.\\n\\n|  |\\n| --- |\\n| ``` from langchain.chains import LLMChainchain = LLMChain(llm=llm, prompt=prompt) # Run the chain only specifying the input variable.print(chain.run(\"colorful socks\")) ``` |\\n\\n|  |\\n| --- |\\n| ``` Colorful Toes Co. ``` |\\n\\nIf there are multiple variables, you can input them all at once using a dictionary.\\n\\n|  |\\n| --- |\\n| ``` prompt = PromptTemplate(    input_variables=[\"company\", \"product\"],    template=\"What is a good name for {company} that makes {product}?\",)chain = LLMChain(llm=llm, prompt=prompt)print(chain.run({    \\'company\\': \"ABC Startup\",    \\'product\\': \"colorful socks\"    })) ``` |\\n\\n|  |\\n| --- |\\n| ``` Socktopia Colourful Creations. ``` |\\n\\nYou can use a chat model in an LLMChain as well:\\n\\n|  |\\n| --- |\\n| ``` from langchain.chat_models import ChatOpenAIfrom langchain.prompts.chat import (    ChatPromptTemplate,    HumanMessagePromptTemplate,)human_message_prompt = HumanMessagePromptTemplate(        prompt=PromptTemplate(            template=\"What is a good name for a company that makes {product}?\",            input_variables=[\"product\"],        )    )chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])chat = ChatOpenAI(temperature=0.9)chain = LLMChain(llm=chat, prompt=chat_prompt_template)print(chain.run(\"colorful socks\")) ``` |\\n\\n|  |\\n| --- |\\n| ``` Rainbow Socks Co. ``` |\\n\\n## Agents\\n\\nSome applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user’s input. In these types of chains, there is a “agent” which has access to a suite of tools. Depending on the user input, the agent can then decide which, if any, of these tools to call.\\n\\nAt the moment, there are two main types of agents in Langchain:\\n\\n1. “Action Agents”: these agents decide an action to take and take that action one step at a time\\n2. “Plan-and-Execute Agents”: these agents first decide a plan of actions to take, and then execute those actions one at a time.\\n\\nWhen should you use each one? Action Agents are more conventional, and good for small tasks. For more complex or long running tasks, the initial planning step helps to maintain long term objectives and focus. However, that comes at the expense of generally more calls and higher latency. These two agents are also not mutually exclusive - in fact, it is often best to have an Action Agent be in charge of the execution for the Plan and Execute agent.\\n\\n### Action Agents\\n\\nHigh level pseudocode of agents looks something like:\\n\\n* Some user input is received\\n* The agent decides which tool - if any - to use, and what the input to that tool should be\\n* That tool is then called with that tool input, and an observation is recorded (this is just the output of calling that tool with that tool input)\\n* That history of tool, tool input, and observation is passed back into the agent, and it decides what step to take next\\n* This is repeated until the agent decides it no longer needs to use a tool, and then it responds directly to the user.\\n\\nThe different abstractions involved in agents are as follows:\\n\\n* Agent: this is where the logic of the application lives. Agents expose an interface that takes in user input along with a list of previous steps the agent has taken, and returns either an AgentAction or AgentFinish  \\n   AgentAction corresponds to the tool to use and the input to that tool\\n\\n  AgentFinish means the agent is done, and has information around what to return to the user\\n* Tools: these are the actions an agent can take. What tools you give an agent highly depend on what you want the agent to do\\n* Toolkits: these are groups of tools designed for a specific use case. For example, in order for an agent to interact with a SQL database in the best way it may need access to one tool to execute queries and another tool to inspect tables.\\n* Agent Executor: this wraps an agent and a list of tools. This is responsible for the loop of running the agent iteratively until the stopping criteria is met.\\n\\nThe most important abstraction of the four above to understand is that of the agent. Although an agent can be defined in whatever way one chooses, the typical way to construct an agent is with:\\n\\n* PromptTemplate: this is responsible for taking the user input and previous steps and constructing a prompt to send to the language model\\n* Language Model: this takes the prompt constructed by the PromptTemplate and returns some output\\n* Output Parser: this takes the output of the Language Model and parses it into an AgentAction or AgentFinish object.\\n\\n### Plan-and-Execute Agents\\n\\nHigh level pseudocode of agents looks something like:\\n\\n* Some user input is received\\n* The planner lists out the steps to take\\n* The executor goes through the list of steps, executing them\\n\\nThe most typical implementation is to have the planner be a language model, and the executor be an action agent.\\n\\n## Reveal the mystery behind agents\\n\\nIt might sound like the agents are so smart, but the power actually comes from the large langage model itself.  \\nWith some clever prompt design, we put the whole workflow to llm through prompting, and let the llm to tell us what to do next.\\n\\nHere we take a custom LLM agent as an example.  \\nAn LLM chat agent consists of three parts:\\n\\n* PromptTemplate: This is the prompt template that can be used to instruct the language model on what to do\\n* ChatModel: This is the language model that powers the agent\\n* stop sequence: Instructs the LLM to stop generating as soon as this string is found\\n* OutputParser: This determines how to parse the LLMOutput into an AgentAction or AgentFinish object\\n\\nThe LLMAgent is used in an AgentExecutor. This AgentExecutor can largely be thought of as a loop that:\\n\\n1. Passes user input and any previous steps to the Agent (in this case, the LLMAgent)\\n2. If the Agent returns an AgentFinish, then return that directly to the user\\n3. If the Agent returns an AgentAction, then use that to call a tool and get an Observation\\n4. Repeat, passing the AgentAction and Observation back to the Agent until an AgentFinish is emitted.\\n\\nAgentAction is a response that consists of action and action\\\\_input. action refers to which tool to use, and action\\\\_input refers to the input to that tool. log can also be provided as more context (that can be used for logging, tracing, etc).\\n\\nAgentFinish is a response that contains the final message to be sent back to the user. This should be used to end an agent run.\\n\\n### Set up environment\\n\\nDo necessary imports, etc.\\n\\n|  |\\n| --- |\\n| ``` !pip install langchain!pip install google-search-results!pip install openai ``` |\\n\\n|  |\\n| --- |\\n| ``` from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParserfrom langchain.prompts import BaseChatPromptTemplatefrom langchain import SerpAPIWrapper, LLMChainfrom langchain.chat_models import ChatOpenAIfrom typing import List, Unionfrom langchain.schema import AgentAction, AgentFinish, HumanMessage import re from getpass import getpass ``` |\\n\\n### Set up tool\\n\\nSet up any tools the agent may want to use. This may be necessary to put in the prompt (so that the agent knows to use these tools).\\n\\n|  |\\n| --- |\\n| ``` SERPAPI_API_KEY = getpass() ``` |\\n\\n|  |\\n| --- |\\n| ``` # Define which tools the agent can use to answer user queriessearch = SerpAPIWrapper(serpapi_api_key=SERPAPI_API_KEY)tools = [    Tool(        name = \"Search\",        func=search.run,        description=\"useful for when you need to answer questions about current events\"    )] ``` |\\n\\n### Prompt Template\\n\\nThis instructs the agent on what to do. Generally, the template should incorporate:\\n\\n* tools: which tools the agent has access and how and when to call them.\\n* intermediate\\\\_steps: These are tuples of previous (AgentAction, Observation) pairs. These are generally not passed directly to the model, but the prompt template formats them in a specific way.\\n* input: generic user input\\n\\n  |  |\\n  | --- |\\n  | ``` # Set up the base templatetemplate = \"\"\"Complete the objective as best you can. You have access to the following tools: {tools} Use the following format: Question: the input question you must answerThought: you should always think about what to doAction: the action to take, should be one of [{tool_names}]Action Input: the input to the actionObservation: the result of the action... (this Thought/Action/Action Input/Observation can repeat N times)Thought: I now know the final answerFinal Answer: the final answer to the original input question These were previous tasks you completed:  Begin! Question: {input}{agent_scratchpad}\"\"\" ``` |\\n\\n  |  |\\n  | --- |\\n  | ``` # Set up a prompt templateclass CustomPromptTemplate(BaseChatPromptTemplate):     # The template to use    template: str     # The list of tools available    tools: List[Tool]         def format_messages(self, **kwargs) -> str:        # Get the intermediate steps (AgentAction, Observation tuples)         # Format them in a particular way        intermediate_steps = kwargs.pop(\"intermediate_steps\")        thoughts = \"\"        for action, observation in intermediate_steps:            thoughts += action.log            thoughts += f\"\\\\nObservation: {observation}\\\\nThought: \"         # Set the agent_scratchpad variable to that value        kwargs[\"agent_scratchpad\"] = thoughts         # Create a tools variable from the list of tools provided        kwargs[\"tools\"] = \"\\\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])         # Create a list of tool names for the tools provided        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])        formatted = self.template.format(**kwargs)        return [HumanMessage(content=formatted)] ``` |\\n\\n  |  |\\n  | --- |\\n  | ``` prompt = CustomPromptTemplate(    template=template,    tools=tools,    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically     # This includes the `intermediate_steps` variable because that is needed    input_variables=[\"input\", \"intermediate_steps\"]) ``` |\\n\\n### Output Parser\\n\\nThe output parser is responsible for parsing the LLM output into AgentAction and AgentFinish. This usually depends heavily on the prompt used.\\n\\nThis is where you can change the parsing to do retries, handle whitespace, etc\\n\\n|  |\\n| --- |\\n| ``` class CustomOutputParser(AgentOutputParser):         def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:         # Check if agent should finish        if \"Final Answer:\" in llm_output:            return AgentFinish(                # Return values is generally always a dictionary with a single `output` key                # It is not recommended to try anything else at the moment :)                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},                log=llm_output,            )         # Parse out the action and action input        regex = r\"Action\\\\s*\\\\d*\\\\s*:(.*?)\\\\nAction\\\\s*\\\\d*\\\\s*Input\\\\s*\\\\d*\\\\s*:[\\\\s]*(.*)\"        match = re.search(regex, llm_output, re.DOTALL)        if not match:            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")        action = match.group(1).strip()        action_input = match.group(2)         # Return the action and action input        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip(\\'\"\\'), log=llm_output) ``` |\\n\\n|  |\\n| --- |\\n| ``` output_parser = CustomOutputParser() ``` |\\n\\n### Set up LLM\\n\\nChoose the LLM you want to use!\\n\\n|  |\\n| --- |\\n| ``` OPENAI_API_KEY = getpass() ``` |\\n\\n|  |\\n| --- |\\n| ``` llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, temperature=0) ``` |\\n\\n### Define the stop sequence\\n\\nThis is important because it tells the LLM when to stop generation.\\n\\nThis depends heavily on the prompt and model you are using. Generally, you want this to be whatever token you use in the prompt to denote the start of an Observation (otherwise, the LLM may hallucinate an observation for you).\\n\\n### Set up the Agent\\n\\nWe can now combine everything to set up our agent\\n\\n|  |\\n| --- |\\n| ``` # LLM chain consisting of the LLM and a promptllm_chain = LLMChain(llm=llm, prompt=prompt) ``` |\\n\\n|  |\\n| --- |\\n| ``` tool_names = [tool.name for tool in tools]agent = LLMSingleActionAgent(    llm_chain=llm_chain,     output_parser=output_parser,    stop=[\"\\\\nObservation:\"],     allowed_tools=tool_names) ``` |\\n\\n### Use the Agent\\n\\nNow we can use it!\\n\\n|  |\\n| --- |\\n| ``` agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)agent_executor.run(\"Search for Leo DiCaprio\\'s girlfriend on the internet.\") ``` |\\n\\n|  |\\n| --- |\\n| ``` > Entering new AgentExecutor chain...Thought: I should use a reliable search engine to get accurate information.Action: SearchAction Input: \"Leo DiCaprio girlfriend\" Observation:He went on to date Gisele Bündchen, Bar Refaeli, Blake Lively, Toni Garrn and Nina Agdal, among others, before finally settling down with current girlfriend Camila Morrone, who is 23 years his junior.I have found the answer to the question.Final Answer: Leo DiCaprio\\'s current girlfriend is Camila Morrone. > Finished chain. ``` |\\n\\n|  |\\n| --- |\\n| ``` \"Leo DiCaprio\\'s current girlfriend is Camila Morrone.\" ``` |\\n\\n---\\n\\n*Author:*\\n\\n[robot learner](/about)\\n\\n*Link:*\\n\\n<https://datasciencebyexample.github.io/2023/05/27/understanding-langchain-chains-and-agents/>\\n\\n*Reprint policy:*\\n\\nAll articles in this blog are used except for special statements\\n[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/deed.zh)\\nreprint policy. If reproduced, please indicate source\\n[robot learner](/about)\\n!\\n\\n[langchain](/tags/langchain/)\\n\\n[#### 微信扫一扫：分享\\n\\n微信扫一扫即可分享！](javascript:)\\n\\nPrevious\\n\\n[How to have good prompt engeering with OpenAI, summary from Andrew NG\\'s course](/2023/05/28/how-to-build-effective-prompts-with-openai/)\\n\\n2023-05-28\\n\\n[data engineering](/categories/data-engineering/)\\n\\n[openai](/tags/openai/)\\n[prompt engeering](/tags/prompt-engeering/)\\n\\nNext\\n\\n[Understanding Class Parameters and Instance Attributes in Python](/2023/05/25/python-class-paramter-vs-instance-attribute/)\\n\\n2023-05-25\\n\\n[data engineering](/categories/data-engineering/)\\n\\n[python](/tags/python/)\\n\\nTOC\\n\\n1. [Chains](#toc-heading-1)\\n   1. [Why do we need chains?](#toc-heading-2)\\n   2. [Quick start: Using LLMChain](#toc-heading-3)\\n2. [Agents](#toc-heading-4)\\n   1. [Action Agents](#toc-heading-5)\\n   2. [Plan-and-Execute Agents](#toc-heading-6)\\n3. [Reveal the mystery behind agents](#toc-heading-7)\\n   1. [Set up environment](#toc-heading-8)\\n   2. [Set up tool](#toc-heading-9)\\n   3. [Prompt Template](#toc-heading-10)\\n   4. [Output Parser](#toc-heading-11)\\n   5. [Set up LLM](#toc-heading-12)\\n   6. [Define the stop sequence](#toc-heading-13)\\n   7. [Set up the Agent](#toc-heading-14)\\n   8. [Use the Agent](#toc-heading-15)'},\n",
       " {'title': 'Tool calling - GitHub Pages',\n",
       "  'url': 'https://langchain-ai.github.io/langgraph/concepts/tools/',\n",
       "  'content': '* Tool calling * Prebuilt tools * Custom tools * Tool execution In these scenarios, tool calling enables models to generate requests that conform to a specified input schema. ## Tool calling¶ Based on the user input and available tools, the model may choose to issue a tool call request. This request is returned in an `AIMessage` object, which includes a `tool_calls` field that specifies the tool name and input arguments: AIMessage(  tool_calls=[  ToolCall(name=\"multiply\", args={\"a\": 2, \"b\": 3}),  ... If the input is unrelated to any tool, the model returns only a natural language message: LangChain provides prebuilt tool integrations for common external systems including APIs, databases, file systems, and web data. *API Reference: tool* ## Tool execution¶ * `ToolNode`: A prebuilt node that executes tools.',\n",
       "  'score': 0.98517,\n",
       "  'raw_content': '[Skip to content](#tools)\\n\\n\\n\\n* [Tool calling](#tool-calling)\\n* [Prebuilt tools](#prebuilt-tools)\\n* [Custom tools](#custom-tools)\\n* [Tool execution](#tool-execution)\\n\\n# Tools[¶](#tools \"Permanent link\")\\n\\nMany AI applications interact with users via natural language. However, some use cases require models to interface directly with external systems—such as APIs, databases, or file systems—using structured input. In these scenarios, [tool calling](../../how-tos/tool-calling/) enables models to generate requests that conform to a specified input schema.\\n\\n**Tools** encapsulate a callable function and its input schema. These can be passed to compatible [chat models](https://python.langchain.com/docs/concepts/chat_models), allowing the model to decide whether to invoke a tool and with what arguments.\\n\\n## Tool calling[¶](#tool-calling \"Permanent link\")\\n\\nTool calling is typically **conditional**. Based on the user input and available tools, the model may choose to issue a tool call request. This request is returned in an `AIMessage` object, which includes a `tool_calls` field that specifies the tool name and input arguments:\\n\\n```\\nllm_with_tools.invoke(\"What is 2 multiplied by 3?\") llm_with_tools. invoke(\"What is 2 multiplied by 3?\")# -> AIMessage(tool_calls=[{\\'name\\': \\'multiply\\', \\'args\\': {\\'a\\': 2, \\'b\\': 3}, ...}]) # -> AIMessage(tool_calls=[{\\'name\\': \\'multiply\\', \\'args\\': {\\'a\\': 2, \\'b\\': 3}, ...}])\\n```\\n\\n```\\nAIMessage(  tool_calls=[  ToolCall(name=\"multiply\", args={\"a\": 2, \"b\": 3}),  ...  ] ) \\n```\\n\\nIf the input is unrelated to any tool, the model returns only a natural language message:\\n\\n```\\nllm_with_tools.invoke(\"Hello world!\") # -> AIMessage(content=\"Hello!\") llm_with_tools. invoke(\"Hello world!\")# -> AIMessage(content=\"Hello!\")\\n```\\n\\nImportantly, the model does not execute the tool—it only generates a request. A separate executor (such as a runtime or agent) is responsible for handling the tool call and returning the result.\\n\\nSee the [tool calling guide](../../how-tos/tool-calling/) for more details.\\n\\n## Prebuilt tools[¶](#prebuilt-tools \"Permanent link\")\\n\\nLangChain provides prebuilt tool integrations for common external systems including APIs, databases, file systems, and web data.\\n\\nBrowse the [integrations directory](https://python.langchain.com/docs/integrations/tools/) for available tools.\\n\\nCommon categories:\\n\\n* **Search**: Bing, SerpAPI, Tavily\\n* **Code execution**: Python REPL, Node.js REPL\\n* **Databases**: SQL, MongoDB, Redis\\n* **Web data**: Scraping and browsing\\n* **APIs**: OpenWeatherMap, NewsAPI, etc.\\n\\n## Custom tools[¶](#custom-tools \"Permanent link\")\\n\\nYou can define custom tools using the `@tool` decorator or plain Python functions. For example:\\n\\n*API Reference: [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)*\\n\\n```\\nfrom langchain_core.tools import tool from  langchain_core.tools  import tool  @tool @tooldef multiply(a: int, b: int) -> int: def  multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\"  \"\"\"Multiply two numbers.\"\"\" return a * b return a* b\\n```\\n\\nSee the [tool calling guide](../../how-tos/tool-calling/) for more details.\\n\\n## Tool execution[¶](#tool-execution \"Permanent link\")\\n\\nWhile the model determines when to call a tool, execution of the tool call must be handled by a runtime component.\\n\\nLangGraph provides prebuilt components for this:\\n\\n* [`ToolNode`](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.tool_node.ToolNode): A prebuilt node that executes tools.\\n* [`create_react_agent`](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent): Constructs a full agent that manages tool calling automatically.\\n\\n '},\n",
       " {'title': 'Call tools - GitHub Pages',\n",
       "  'url': 'https://langchain-ai.github.io/langgraph/how-tos/tool-calling/',\n",
       "  'content': 'from langchain_core.tools import tool from  langchain_core.tools  import toolfrom langgraph.prebuilt import create_react_agent from langgraph.prebuilt import create_react_agent from  langgraph.prebuilt  import create_react_agent  @tool @tooldef multiply(a: int, b: int) -> int: def  multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\"  \"\"\"Multiply two numbers.\"\"\" return a * b return a* b  agent = create_react_agent( agent = create_react_agent( agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet\", model =\"anthropic:claude-3-7-sonnet\", tools=[multiply] tools =[multiply]) )agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what\\'s 42 x 7?\"}]}) agent. custom_tool_node = ToolNode( custom_tool_node = ToolNode( [multiply], [multiply], handle_tool_errors=\"Cannot use 42 as a first operand!\" handle_tool_errors =\"Cannot use 42 as a first operand!\") )  agent_custom = create_react_agent( agent_custom = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", model =\"anthropic:claude-3-7-sonnet-latest\", tools=custom_tool_node tools = custom_tool_node) )  agent_custom.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what\\'s 42 x 7?\"}]}) agent_custom.',\n",
       "  'score': 0.98376,\n",
       "  'raw_content': '[Skip to content](#call-tools)\\n\\n\\n\\n* [Define a tool](#define-a-tool)\\n* [Run a tool](#run-a-tool)\\n* [Use in an agent](#use-in-an-agent) \\n\\n  + [Dynamically select tools](#dynamically-select-tools)\\n* [Use in a workflow](#use-in-a-workflow) \\n\\n  + [ToolNode](#toolnode)\\n* [Tool customization](#tool-customization) \\n\\n  + [Parameter descriptions](#parameter-descriptions)\\n  + [Explicit input schema](#explicit-input-schema)\\n  + [Tool name](#tool-name)\\n* [Context management](#context-management) \\n\\n  + [Configuration](#configuration)\\n  + [Short-term memory](#short-term-memory)\\n  + [Long-term memory](#long-term-memory)\\n* [Advanced tool features](#advanced-tool-features) \\n\\n  + [Immediate return](#immediate-return)\\n  + [Force tool use](#force-tool-use)\\n  + [Disable parallel calls](#disable-parallel-calls)\\n  + [Handle errors](#handle-errors) \\n\\n    - [Disable error handling](#disable-error-handling)\\n    - [Custom error messages](#custom-error-messages)\\n    - [Error handling in agents](#error-handling-in-agents)\\n  + [Handle large numbers of tools](#handle-large-numbers-of-tools)\\n* [Prebuilt tools](#prebuilt-tools) \\n\\n  + [LLM provider tools](#llm-provider-tools)\\n  + [LangChain tools](#langchain-tools)\\n\\n# Call tools[¶](#call-tools \"Permanent link\")\\n\\n[Tools](../../concepts/tools/) encapsulate a callable function and its input schema. These can be passed to compatible chat models, allowing the model to decide whether to invoke a tool and determine the appropriate arguments.\\n\\nYou can [define your own tools](#define-a-tool) or use [prebuilt tools](#prebuilt-tools)\\n\\n## Define a tool[¶](#define-a-tool \"Permanent link\")\\n\\nDefine a basic tool with the [@tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) decorator:\\n\\n*API Reference: [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)*\\n\\n```\\nfrom langchain_core.tools import tool from  langchain_core.tools  import tool  @tool @tool @tooldef multiply(a: int, b: int) -> int: def  multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\"  \"\"\"Multiply two numbers.\"\"\" return a * b return a* b\\n```\\n\\n## Run a tool[¶](#run-a-tool \"Permanent link\")\\n\\nTools conform to the [Runnable interface](https://python.langchain.com/docs/concepts/runnables/), which means you can run a tool using the `invoke` method:\\n\\n```\\nmultiply.invoke({\"a\": 6, \"b\": 7}) # returns 42 multiply. invoke({\"a\": 6, \"b\": 7}) # returns 42\\n```\\n\\nIf the tool is invoked with `type=\"tool_call\"`, it will return a [ToolMessage](https://python.langchain.com/docs/concepts/messages/#toolmessage):\\n\\n```\\ntool_call = { tool_call ={ \"type\": \"tool_call\", \"type\": \"tool_call\", \"id\": \"1\", \"id\": \"1\", \"args\": {\"a\": 42, \"b\": 7} \"args\":{\"a\": 42, \"b\": 7}} }multiply.invoke(tool_call) # returns a ToolMessage object multiply. invoke(tool_call) # returns a ToolMessage object\\n```\\n\\nOutput:\\n\\n```\\nToolMessage(content=\\'294\\', name=\\'multiply\\', tool_call_id=\\'1\\') ToolMessage(content=\\'294\\', name=\\'multiply\\', tool_call_id=\\'1\\')\\n```\\n\\n## Use in an agent[¶](#use-in-an-agent \"Permanent link\")\\n\\nTo create a tool-calling agent, you can use the prebuilt [create\\\\_react\\\\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent):\\n\\n*API Reference: [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [create\\\\_react\\\\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)*\\n\\n```\\nfrom langchain_core.tools import tool from  langchain_core.tools  import toolfrom langgraph.prebuilt import create_react_agent from langgraph.prebuilt import create_react_agent from  langgraph.prebuilt  import create_react_agent  @tool @tooldef multiply(a: int, b: int) -> int: def  multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\"  \"\"\"Multiply two numbers.\"\"\" return a * b return a* b  agent = create_react_agent( agent = create_react_agent( agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet\", model =\"anthropic:claude-3-7-sonnet\", tools=[multiply] tools =[multiply]) )agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what\\'s 42 x 7?\"}]}) agent. invoke({\"messages\":[{\"role\": \"user\", \"content\": \"what\\'s 42 x 7?\"}]})\\n```\\n\\n### Dynamically select tools[¶](#dynamically-select-tools \"Permanent link\")\\n\\nConfigure tool availability at runtime based on context:\\n\\n*API Reference: [init\\\\_chat\\\\_model](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [create\\\\_react\\\\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) | [AgentState](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.AgentState)*\\n\\n```\\nfrom dataclasses import dataclass from  dataclasses  import dataclass from typing import Literal from  typing  import Literal  from langchain.chat_models import init_chat_model from  langchain.chat_models  import init_chat_modelfrom langchain_core.tools import tool from  langchain_core.tools  import tool  from langgraph.prebuilt import create_react_agent from  langgraph.prebuilt  import create_react_agentfrom langgraph.prebuilt.chat_agent_executor import AgentState from  langgraph.prebuilt.chat_agent_executor  import AgentStatefrom langgraph.runtime import Runtime from  langgraph.runtime  import Runtime   @dataclass @dataclassclass CustomContext: class  CustomContext: tools: list[Literal[\"weather\", \"compass\"]] tools: list[Literal[\"weather\", \"compass\"]]   @tool @tooldef weather() -> str: def  weather() -> str: \"\"\"Returns the current weather conditions.\"\"\"  \"\"\"Returns the current weather conditions.\"\"\" return \"It\\'s nice and sunny.\" return\"It\\'s nice and sunny.\"   @tool @tooldef compass() -> str: def  compass() -> str: \"\"\"Returns the direction the user is facing.\"\"\"  \"\"\"Returns the direction the user is facing.\"\"\"  return \"North\" return \"North\"  model = init_chat_model(\"anthropic:claude-sonnet-4-20250514\") model = init_chat_model(\"anthropic:claude-sonnet-4-20250514\")  def configure_model(state: AgentState, runtime: Runtime[CustomContext]): def configure_model(state: AgentState, runtime: Runtime[CustomContext]): def  configure_model(state: AgentState, runtime: Runtime[CustomContext]): \"\"\"Configure the model with tools based on runtime context.\"\"\"  \"\"\"Configure the model with tools based on runtime context.\"\"\" selected_tools = [ selected_tools =[ tool tool for tool in [weather, compass] for tool in[weather, compass] if tool.name in runtime.context.tools if tool. name in runtime. context. tools ] ] return model.bind_tools(selected_tools) return model. bind_tools(selected_tools)   agent = create_react_agent( agent = create_react_agent( # Dynamically configure the model with tools based on runtime context # Dynamically configure the model with tools based on runtime context configure_model,  configure_model, configure_model,  # Initialize with all tools available # Initialize with all tools available tools=[weather, compass]  tools=[weather, compass] tools =[weather, compass]) )  output = agent.invoke( output = agent. invoke( { { \"messages\": [ \"messages\":[ { { \"role\": \"user\", \"role\": \"user\", \"content\": \"Who are you and what tools do you have access to?\", \"content\": \"Who are you and what tools do you have access to?\", } } ] ] }, }, context=CustomContext(tools=[\"weather\"]), # Only enable the weather tool  context=CustomContext(tools=[\"weather\"]), # Only enable the weather tool context = CustomContext(tools =[\"weather\"]), # Only enable the weather tool) )  print(output[\"messages\"][-1].text()) print(output[\"messages\"][- 1]. text())\\n```\\n\\nAdded in version 0.6.0\\n\\n## Use in a workflow[¶](#use-in-a-workflow \"Permanent link\")\\n\\nIf you are writing a custom workflow, you will need to:\\n\\n1. register the tools with the chat model\\n2. call the tool if the model decides to use it\\n\\nUse `model.bind_tools()` to register the tools with the model.\\n\\n*API Reference: [init\\\\_chat\\\\_model](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html)*\\n\\n```\\nfrom langchain.chat_models import init_chat_model from  langchain.chat_models  import init_chat_model  model = init_chat_model(model=\"claude-3-5-haiku-latest\") model = init_chat_model(model =\"claude-3-5-haiku-latest\")  model_with_tools = model.bind_tools([multiply]) model_with_tools = model.bind_tools([multiply]) model_with_tools = model. bind_tools([multiply])\\n```\\n\\nLLMs automatically determine if a tool invocation is necessary and handle calling the tool with the appropriate arguments.\\n\\n Extended example: attach tools to a chat model\\n\\n```\\nfrom langchain_core.tools import tool from  langchain_core.tools  import toolfrom langchain.chat_models import init_chat_model from  langchain.chat_models  import init_chat_model  @tool @tooldef multiply(a: int, b: int) -> int: def  multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\"  \"\"\"Multiply two numbers.\"\"\" return a * b return a* b  model = init_chat_model(model=\"claude-3-5-haiku-latest\") model = init_chat_model(model =\"claude-3-5-haiku-latest\")model_with_tools = model.bind_tools([multiply]) model_with_tools = model.bind_tools([multiply]) model_with_tools = model. bind_tools([multiply])  response_message = model_with_tools.invoke(\"what\\'s 42 x 7?\") response_message = model_with_tools. invoke(\"what\\'s 42 x 7?\")tool_call = response_message.tool_calls[0] tool_call = response_message. tool_calls[0]  multiply.invoke(tool_call) multiply. invoke(tool_call)\\n```\\n\\n```\\nToolMessage( ToolMessage( content=\\'294\\',  content=\\'294\\', name=\\'multiply\\',  name=\\'multiply\\', tool_call_id=\\'toolu_0176DV4YKSD8FndkeuuLj36c\\'  tool_call_id=\\'toolu_0176DV4YKSD8FndkeuuLj36c\\') )\\n```\\n\\n#### ToolNode[¶](#toolnode \"Permanent link\")\\n\\nTo execute tools in custom workflows, use the prebuilt [`ToolNode`](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.tool_node.ToolNode) or implement your own custom node.\\n\\n`ToolNode` is a specialized node for executing tools in a workflow. It provides the following features:\\n\\n* Supports both synchronous and asynchronous tools.\\n* Executes multiple tools concurrently.\\n* Handles errors during tool execution (`handle_tool_errors=True`, enabled by default). See [handling tool errors](#handle-errors) for more details.\\n\\n`ToolNode` operates on [`MessagesState`](../../concepts/low_level/#messagesstate):\\n\\n* **Input**: `MessagesState`, where the last message is an `AIMessage` containing the `tool_calls` parameter.\\n* **Output**: `MessagesState` updated with the resulting [`ToolMessage`](https://python.langchain.com/docs/concepts/messages/#toolmessage) from executed tools.\\n\\n*API Reference: [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)*\\n\\n```\\nfrom langgraph.prebuilt import ToolNode from langgraph.prebuilt import ToolNode from  langgraph.prebuilt  import ToolNode  def get_weather(location: str): def  get_weather(location: str): \"\"\"Call to get the current weather.\"\"\"  \"\"\"Call to get the current weather.\"\"\" if location.lower() in [\"sf\", \"san francisco\"]: if location. lower() in[\"sf\", \"san francisco\"]: return \"It\\'s 60 degrees and foggy.\" return\"It\\'s 60 degrees and foggy.\" else: else: return \"It\\'s 90 degrees and sunny.\" return\"It\\'s 90 degrees and sunny.\"  def get_coolest_cities(): def  get_coolest_cities():  \"\"\"Get a list of coolest cities\"\"\"  \"\"\"Get a list of coolest cities\"\"\" return \"nyc, sf\" return\"nyc, sf\"  tool_node = ToolNode([get_weather, get_coolest_cities]) tool_node = ToolNode([get_weather, get_coolest_cities]) tool_node = ToolNode([get_weather, get_coolest_cities])tool_node.invoke({\"messages\": [...]}) tool_node. invoke({\"messages\":[...]})\\n```\\n\\n Single tool call\\n\\n```\\nfrom langchain_core.messages import AIMessage from  langchain_core.messages  import AIMessagefrom langgraph.prebuilt import ToolNode from  langgraph.prebuilt  import ToolNode  # Define tools # Define tools @tool @tooldef get_weather(location: str): def  get_weather(location: str): \"\"\"Call to get the current weather.\"\"\"  \"\"\"Call to get the current weather.\"\"\" if location.lower() in [\"sf\", \"san francisco\"]: if location. lower() in[\"sf\", \"san francisco\"]: return \"It\\'s 60 degrees and foggy.\" return\"It\\'s 60 degrees and foggy.\" else: else: return \"It\\'s 90 degrees and sunny.\" return\"It\\'s 90 degrees and sunny.\"  tool_node = ToolNode([get_weather]) tool_node = ToolNode([get_weather]) tool_node = ToolNode([get_weather])  message_with_single_tool_call = AIMessage( message_with_single_tool_call = AIMessage( content=\"\", content = \"\", tool_calls=[ tool_calls =[ { { \"name\": \"get_weather\", \"name\": \"get_weather\", \"args\": {\"location\": \"sf\"}, \"args\":{\"location\": \"sf\"}, \"id\": \"tool_call_id\", \"id\": \"tool_call_id\", \"type\": \"tool_call\", \"type\": \"tool_call\", } } ], ],) )  tool_node.invoke({\"messages\": [message_with_single_tool_call]}) tool_node. invoke({\"messages\":[message_with_single_tool_call]})\\n```\\n\\n```\\n{\\'messages\\': [ToolMessage(content=\"It\\'s 60 degrees and foggy.\", name=\\'get_weather\\', tool_call_id=\\'tool_call_id\\')]} \\n```\\n\\n  Multiple tool calls\\n\\n```\\nfrom langchain_core.messages import AIMessage from  langchain_core.messages  import AIMessagefrom langgraph.prebuilt import ToolNode from  langgraph.prebuilt  import ToolNode  # Define tools # Define tools  def get_weather(location: str): def  get_weather(location: str): \"\"\"Call to get the current weather.\"\"\"  \"\"\"Call to get the current weather.\"\"\" if location.lower() in [\"sf\", \"san francisco\"]: if location. lower() in[\"sf\", \"san francisco\"]: return \"It\\'s 60 degrees and foggy.\" return\"It\\'s 60 degrees and foggy.\" else: else: return \"It\\'s 90 degrees and sunny.\" return\"It\\'s 90 degrees and sunny.\"  def get_coolest_cities(): def  get_coolest_cities():  \"\"\"Get a list of coolest cities\"\"\"  \"\"\"Get a list of coolest cities\"\"\" return \"nyc, sf\" return\"nyc, sf\"  tool_node = ToolNode([get_weather, get_coolest_cities]) tool_node = ToolNode([get_weather, get_coolest_cities]) tool_node = ToolNode([get_weather, get_coolest_cities])  message_with_multiple_tool_calls = AIMessage( message_with_multiple_tool_calls = AIMessage( content=\"\", content = \"\", tool_calls=[ tool_calls =[ { { \"name\": \"get_coolest_cities\", \"name\": \"get_coolest_cities\", \"args\": {}, \"args\":{}, \"id\": \"tool_call_id_1\", \"id\": \"tool_call_id_1\", \"type\": \"tool_call\", \"type\": \"tool_call\", }, }, { { \"name\": \"get_weather\", \"name\": \"get_weather\", \"args\": {\"location\": \"sf\"}, \"args\":{\"location\": \"sf\"}, \"id\": \"tool_call_id_2\", \"id\": \"tool_call_id_2\", \"type\": \"tool_call\", \"type\": \"tool_call\", }, }, ], ],) )  tool_node.invoke({\"messages\": [message_with_multiple_tool_calls]}) # (1)! tool_node.invoke({\"messages\": [message_with_multiple_tool_calls]}) # (1)! tool_node. invoke({\"messages\":[message_with_multiple_tool_calls]})# (1)!\\n```\\n\\n1. `ToolNode` will execute both tools in parallel\\n\\n```\\n{  \\'messages\\': [  ToolMessage(content=\\'nyc, sf\\', name=\\'get_coolest_cities\\', tool_call_id=\\'tool_call_id_1\\'),  ToolMessage(content=\"It\\'s 60 degrees and foggy.\", name=\\'get_weather\\', tool_call_id=\\'tool_call_id_2\\')  ] } \\n```\\n\\n  Use with a chat model\\n\\n```\\nfrom langchain.chat_models import init_chat_model from  langchain.chat_models  import init_chat_modelfrom langgraph.prebuilt import ToolNode from  langgraph.prebuilt  import ToolNode  def get_weather(location: str): def  get_weather(location: str): \"\"\"Call to get the current weather.\"\"\"  \"\"\"Call to get the current weather.\"\"\" if location.lower() in [\"sf\", \"san francisco\"]: if location. lower() in[\"sf\", \"san francisco\"]: return \"It\\'s 60 degrees and foggy.\" return\"It\\'s 60 degrees and foggy.\" else: else: return \"It\\'s 90 degrees and sunny.\" return\"It\\'s 90 degrees and sunny.\"  tool_node = ToolNode([get_weather]) tool_node = ToolNode([get_weather]) tool_node = ToolNode([get_weather])  model = init_chat_model(model=\"claude-3-5-haiku-latest\") model = init_chat_model(model =\"claude-3-5-haiku-latest\")model_with_tools = model.bind_tools([get_weather]) # (1)! model_with_tools = model.bind_tools([get_weather]) # (1)! model_with_tools = model. bind_tools([get_weather])# (1)!   response_message = model_with_tools.invoke(\"what\\'s the weather in sf?\") response_message = model_with_tools.invoke(\"what\\'s the weather in sf?\") response_message = model_with_tools. invoke(\"what\\'s the weather in sf?\")tool_node.invoke({\"messages\": [response_message]}) tool_node. invoke({\"messages\":[response_message]})\\n```\\n\\n1. Use `.bind_tools()` to attach the tool schema to the chat model\\n\\n```\\n{\\'messages\\': [ToolMessage(content=\"It\\'s 60 degrees and foggy.\", name=\\'get_weather\\', tool_call_id=\\'toolu_01Pnkgw5JeTRxXAU7tyHT4UW\\')]} \\n```\\n\\n  Use in a tool-calling agent\\n\\nThis is an example of creating a tool-calling agent from scratch using `ToolNode`. You can also use LangGraph\\'s prebuilt [agent](../../agents/agents/).\\n\\n```\\nfrom langchain.chat_models import init_chat_model from  langchain.chat_models  import init_chat_modelfrom langgraph.prebuilt import ToolNode from  langgraph.prebuilt  import ToolNodefrom langgraph.graph import StateGraph, MessagesState, START, END from  langgraph.graph  import StateGraph, MessagesState, START, END  def get_weather(location: str): def  get_weather(location: str): \"\"\"Call to get the current weather.\"\"\"  \"\"\"Call to get the current weather.\"\"\" if location.lower() in [\"sf\", \"san francisco\"]: if location. lower() in[\"sf\", \"san francisco\"]: return \"It\\'s 60 degrees and foggy.\" return\"It\\'s 60 degrees and foggy.\" else: else: return \"It\\'s 90 degrees and sunny.\" return\"It\\'s 90 degrees and sunny.\"  tool_node = ToolNode([get_weather]) tool_node = ToolNode([get_weather]) tool_node = ToolNode([get_weather])  model = init_chat_model(model=\"claude-3-5-haiku-latest\") model = init_chat_model(model =\"claude-3-5-haiku-latest\")model_with_tools = model.bind_tools([get_weather]) model_with_tools = model.bind_tools([get_weather]) model_with_tools = model. bind_tools([get_weather])  def should_continue(state: MessagesState): def  should_continue(state: MessagesState): messages = state[\"messages\"] messages = state[\"messages\"] last_message = messages[-1] last_message = messages[- 1] if last_message.tool_calls: if last_message. tool_calls:  return \"tools\" return \"tools\"  return END return END  def call_model(state: MessagesState): def  call_model(state: MessagesState): messages = state[\"messages\"] messages = state[\"messages\"] response = model_with_tools.invoke(messages) response = model_with_tools. invoke(messages) return {\"messages\": [response]} return{\"messages\":[response]}  builder = StateGraph(MessagesState) builder = StateGraph(MessagesState)  # Define the two nodes we will cycle between # Define the two nodes we will cycle betweenbuilder.add_node(\"call_model\", call_model) builder. add_node(\"call_model\", call_model)builder.add_node(\"tools\", tool_node) builder.add_node(\"tools\", tool_node) builder. add_node(\"tools\", tool_node)  builder.add_edge(START, \"call_model\") builder. add_edge(START, \"call_model\")builder.add_conditional_edges(\"call_model\", should_continue, [\"tools\", END]) builder. add_conditional_edges(\"call_model\", should_continue,[\"tools\", END])builder.add_edge(\"tools\", \"call_model\") builder. add_edge(\"tools\", \"call_model\")  graph = builder.compile() graph = builder. compile()  graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what\\'s the weather in sf?\"}]}) graph. invoke({\"messages\":[{\"role\": \"user\", \"content\": \"what\\'s the weather in sf?\"}]})\\n```\\n\\n```\\n{  \\'messages\\': [  HumanMessage(content=\"what\\'s the weather in sf?\"),  AIMessage(  content=[{\\'text\\': \"I\\'ll help you check the weather in San Francisco right now.\", \\'type\\': \\'text\\'}, {\\'id\\': \\'toolu_01A4vwUEgBKxfFVc5H3v1CNs\\', \\'input\\': {\\'location\\': \\'San Francisco\\'}, \\'name\\': \\'get_weather\\', \\'type\\': \\'tool_use\\'}],  tool_calls=[{\\'name\\': \\'get_weather\\', \\'args\\': {\\'location\\': \\'San Francisco\\'}, \\'id\\': \\'toolu_01A4vwUEgBKxfFVc5H3v1CNs\\', \\'type\\': \\'tool_call\\'}]  ),  ToolMessage(content=\"It\\'s 60 degrees and foggy.\"),  AIMessage(content=\"The current weather in San Francisco is 60 degrees and foggy. Typical San Francisco weather with its famous marine layer!\")  ] } \\n```\\n\\n## Tool customization[¶](#tool-customization \"Permanent link\")\\n\\nFor more control over tool behavior, use the `@tool` decorator.\\n\\n### Parameter descriptions[¶](#parameter-descriptions \"Permanent link\")\\n\\nAuto-generate descriptions from docstrings:\\n\\n*API Reference: [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)*\\n\\n```\\nfrom langchain_core.tools import tool from langchain_core.tools import tool from  langchain_core.tools  import tool  @tool(\"multiply_tool\", parse_docstring=True) @tool(\"multiply_tool\", parse_docstring=True) @tool(\"multiply_tool\", parse_docstring = True)def multiply(a: int, b: int) -> int: def  multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.  \"\"\"Multiply two numbers.   Args:  Args: a: First operand  a: First operand b: Second operand  b: Second operand  \"\"\"  \"\"\" return a * b return a* b\\n```\\n\\n### Explicit input schema[¶](#explicit-input-schema \"Permanent link\")\\n\\nDefine schemas using `args_schema`:\\n\\n*API Reference: [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)*\\n\\n```\\nfrom pydantic import BaseModel, Field from  pydantic  import BaseModel, Fieldfrom langchain_core.tools import tool from  langchain_core.tools  import tool  class MultiplyInputSchema(BaseModel): class  MultiplyInputSchema(BaseModel):  \"\"\"Multiply two numbers\"\"\"  \"\"\"Multiply two numbers\"\"\" a: int = Field(description=\"First operand\") a: int = Field(description = \"First operand\") b: int = Field(description=\"Second operand\") b: int = Field(description = \"Second operand\")  @tool(\"multiply_tool\", args_schema=MultiplyInputSchema) @tool(\"multiply_tool\", args_schema=MultiplyInputSchema) @tool(\"multiply_tool\", args_schema = MultiplyInputSchema)def multiply(a: int, b: int) -> int: def  multiply(a: int, b: int) -> int: return a * b return a* b\\n```\\n\\n### Tool name[¶](#tool-name \"Permanent link\")\\n\\nOverride the default tool name using the first argument or name property:\\n\\n*API Reference: [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)*\\n\\n```\\nfrom langchain_core.tools import tool from  langchain_core.tools  import tool  @tool(\"multiply_tool\") @tool(\"multiply_tool\") @tool(\"multiply_tool\")def multiply(a: int, b: int) -> int: def  multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\"  \"\"\"Multiply two numbers.\"\"\" return a * b return a* b\\n```\\n\\n## Context management[¶](#context-management \"Permanent link\")\\n\\nTools within LangGraph sometimes require context data, such as runtime-only arguments (e.g., user IDs or session details), that should not be controlled by the model. LangGraph provides three methods for managing such context:\\n\\n| Type | Usage Scenario | Mutable | Lifetime |\\n| --- | --- | --- | --- |\\n| [Configuration](#configuration) | Static, immutable runtime data | ❌ | Single invocation |\\n| [Short-term memory](#short-term-memory) | Dynamic, changing data during invocation | ✅ | Single invocation |\\n| [Long-term memory](#long-term-memory) | Persistent, cross-session data | ✅ | Across multiple sessions |\\n\\n### Configuration[¶](#configuration \"Permanent link\")\\n\\nUse configuration when you have **immutable** runtime data that tools require, such as user identifiers. You pass these arguments via [`RunnableConfig`](https://python.langchain.com/docs/concepts/runnables/#runnableconfig) at invocation and access them in the tool:\\n\\n*API Reference: [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html)*\\n\\n```\\nfrom langchain_core.tools import tool from  langchain_core.tools  import toolfrom langchain_core.runnables import RunnableConfig from  langchain_core.runnables  import RunnableConfig  @tool @tooldef get_user_info(config: RunnableConfig) -> str: def get_user_info(config: RunnableConfig) -> str: def  get_user_info(config: RunnableConfig) -> str: \"\"\"Retrieve user information based on user ID.\"\"\"  \"\"\"Retrieve user information based on user ID.\"\"\" user_id = config[\"configurable\"].get(\"user_id\") user_id = config[\"configurable\"]. get(\"user_id\") return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\" return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\"  # Invocation example with an agent # Invocation example with an agentagent.invoke( agent. invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"look up user info\"}]}, {\"messages\":[{\"role\": \"user\", \"content\": \"look up user info\"}]}, config={\"configurable\": {\"user_id\": \"user_123\"}}  config={\"configurable\": {\"user_id\": \"user_123\"}} config ={\"configurable\":{\"user_id\": \"user_123\"}}) )\\n```\\n\\n Extended example: Access config in tools\\n\\n```\\nfrom langchain_core.runnables import RunnableConfig from  langchain_core.runnables  import RunnableConfigfrom langchain_core.tools import tool from  langchain_core.tools  import toolfrom langgraph.prebuilt import create_react_agent from  langgraph.prebuilt  import create_react_agent  def get_user_info( def  get_user_info( config: RunnableConfig,  config: RunnableConfig, config: RunnableConfig,) -> str: ) -> str: \"\"\"Look up user info.\"\"\"  \"\"\"Look up user info.\"\"\" user_id = config[\"configurable\"].get(\"user_id\")  user_id = config[\"configurable\"].get(\"user_id\") user_id = config[\"configurable\"]. get(\"user_id\") return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\" return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\"  agent = create_react_agent( agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", model =\"anthropic:claude-3-7-sonnet-latest\", tools=[get_user_info], tools =[get_user_info],) )  agent.invoke( agent. invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]}, {\"messages\":[{\"role\": \"user\", \"content\": \"look up user information\"}]}, config={\"configurable\": {\"user_id\": \"user_123\"}}  config={\"configurable\": {\"user_id\": \"user_123\"}} config ={\"configurable\":{\"user_id\": \"user_123\"}}) )\\n```\\n\\n### Short-term memory[¶](#short-term-memory \"Permanent link\")\\n\\nShort-term memory maintains **dynamic** state that changes during a single execution.\\n\\nTo **access** (read) the graph state inside the tools, you can use a special parameter **annotation** — [`InjectedState`](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.tool_node.InjectedState):\\n\\n*API Reference: [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [InjectedState](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.InjectedState) | [create\\\\_react\\\\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) | [AgentState](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.AgentState)*\\n\\n```\\nfrom typing import Annotated, NotRequired from  typing  import Annotated, NotRequiredfrom langchain_core.tools import tool from  langchain_core.tools  import toolfrom langgraph.prebuilt import InjectedState, create_react_agent from  langgraph.prebuilt  import InjectedState, create_react_agentfrom langgraph.prebuilt.chat_agent_executor import AgentState from  langgraph.prebuilt.chat_agent_executor  import AgentState  class CustomState(AgentState): class  CustomState(AgentState): # The user_name field in short-term state # The user_name field in short-term state user_name: NotRequired[str] user_name: NotRequired[str]  @tool @tooldef get_user_name( def  get_user_name( state: Annotated[CustomState, InjectedState]  state: Annotated[CustomState, InjectedState] state: Annotated[CustomState, InjectedState]) -> str: ) -> str: \"\"\"Retrieve the current user-name from state.\"\"\"  \"\"\"Retrieve the current user-name from state.\"\"\"  # Return stored name or a default if not set # Return stored name or a default if not set return state.get(\"user_name\", \"Unknown user\") return state. get(\"user_name\", \"Unknown user\")  # Example agent setup # Example agent setupagent = create_react_agent( agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", model =\"anthropic:claude-3-7-sonnet-latest\", tools=[get_user_name], tools =[get_user_name], state_schema=CustomState, state_schema = CustomState,) )  # Invocation: reads the name from state (initially empty) # Invocation: reads the name from state (initially empty)agent.invoke({\"messages\": \"what\\'s my name?\"}) agent. invoke({\"messages\": \"what\\'s my name?\"})\\n```\\n\\nUse a tool that returns a `Command` to **update** `user_name` and append a confirmation message:\\n\\n*API Reference: [Command](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command) | [ToolMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [InjectedToolCallId](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.InjectedToolCallId.html)*\\n\\n```\\nfrom typing import Annotated from  typing  import Annotatedfrom langgraph.types import Command from  langgraph.types  import Commandfrom langchain_core.messages import ToolMessage from  langchain_core.messages  import ToolMessagefrom langchain_core.tools import tool, InjectedToolCallId from  langchain_core.tools  import tool, InjectedToolCallId  @tool @tooldef update_user_name( def  update_user_name( new_name: str, new_name: str, tool_call_id: Annotated[str, InjectedToolCallId] tool_call_id: Annotated[str, InjectedToolCallId]) -> Command: ) -> Command: \"\"\"Update user-name in short-term memory.\"\"\"  \"\"\"Update user-name in short-term memory.\"\"\" return Command(update={  return Command(update={ return Command(update ={ \"user_name\": new_name,  \"user_name\": new_name, \"user_name\": new_name, \"messages\": [  \"messages\": [ \"messages\":[ ToolMessage(f\"Updated user name to {new_name}\", tool_call_id=tool_call_id)  ToolMessage(f\"Updated user name to {new_name}\", tool_call_id=tool_call_id) ToolMessage(f \"Updated user name to {new_name} \", tool_call_id = tool_call_id) ]  ] ] })  }) })\\n```\\n\\nImportant\\n\\nIf you want to use tools that return `Command` and update graph state, you can either use prebuilt [`create_react_agent`](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) / [`ToolNode`](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.tool_node.ToolNode) components, or implement your own tool-executing node that collects `Command` objects returned by the tools and returns a list of them, e.g.:\\n\\n```\\ndef call_tools(state): def  call_tools(state): ... ... commands = [tools_by_name[tool_call[\"name\"]].invoke(tool_call) for tool_call in tool_calls] commands =[tools_by_name[tool_call[\"name\"]]. invoke(tool_call) for tool_call in tool_calls]  return commands return commands\\n```\\n\\n### Long-term memory[¶](#long-term-memory \"Permanent link\")\\n\\nUse [long-term memory](../../concepts/memory/#long-term-memory) to store user-specific or application-specific data across conversations. This is useful for applications like chatbots, where you want to remember user preferences or other information.\\n\\nTo use long-term memory, you need to:\\n\\n1. [Configure a store](../memory/add-memory/#add-long-term-memory) to persist data across invocations.\\n2. Access the store from within tools.\\n\\nTo **access** information in the store:\\n\\n*API Reference: [RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [get\\\\_store](https://langchain-ai.github.io/langgraph/reference/config/#langgraph.config.get_store)*\\n\\n```\\nfrom langchain_core.runnables import RunnableConfig from  langchain_core.runnables  import RunnableConfigfrom langchain_core.tools import tool from  langchain_core.tools  import toolfrom langgraph.graph import StateGraph from  langgraph.graph  import StateGraphfrom langgraph.config import get_store from langgraph.config import get_store from  langgraph.config  import get_store  @tool @tooldef get_user_info(config: RunnableConfig) -> str: def  get_user_info(config: RunnableConfig) -> str: \"\"\"Look up user info.\"\"\"  \"\"\"Look up user info.\"\"\" # Same as that provided to `builder.compile(store=store)` # Same as that provided to `builder.compile(store=store)`  # or `create_react_agent` # or `create_react_agent` store = get_store()  store = get_store() store = get_store() user_id = config[\"configurable\"].get(\"user_id\") user_id = config[\"configurable\"]. get(\"user_id\") user_info = store.get((\"users\",), user_id)  user_info = store.get((\"users\",), user_id) user_info = store. get((\"users\",), user_id) return str(user_info.value) if user_info else \"Unknown user\" return str(user_info. value) if user_info else \"Unknown user\"  builder = StateGraph(...) builder = StateGraph(...)... ...graph = builder.compile(store=store) graph = builder. compile(store = store)\\n```\\n\\n Access long-term memory\\n\\n```\\nfrom langchain_core.runnables import RunnableConfig from  langchain_core.runnables  import RunnableConfigfrom langchain_core.tools import tool from  langchain_core.tools  import toolfrom langgraph.config import get_store from  langgraph.config  import get_storefrom langgraph.prebuilt import create_react_agent from  langgraph.prebuilt  import create_react_agentfrom langgraph.store.memory import InMemoryStore from  langgraph.store.memory  import InMemoryStore  store = InMemoryStore() # (1)! store = InMemoryStore() # (1)! store = InMemoryStore()# (1)!  store.put( # (2)! store.put( # (2)! store. put(# (2)! (\"users\",), # (3)! (\"users\",),# (3)! \"user_123\", # (4)! \"user_123\",# (4)! { { \"name\": \"John Smith\", \"name\": \"John Smith\", \"language\": \"English\", \"language\": \"English\", } # (5)! }# (5)!) )  @tool @tooldef get_user_info(config: RunnableConfig) -> str: def  get_user_info(config: RunnableConfig) -> str: \"\"\"Look up user info.\"\"\"  \"\"\"Look up user info.\"\"\"  # Same as that provided to `create_react_agent` # Same as that provided to `create_react_agent` store = get_store() # (6)!  store = get_store() # (6)! store = get_store()# (6)! user_id = config[\"configurable\"].get(\"user_id\") user_id = config[\"configurable\"]. get(\"user_id\") user_info = store.get((\"users\",), user_id) # (7)!  user_info = store.get((\"users\",), user_id) # (7)! user_info = store. get((\"users\",), user_id)# (7)! return str(user_info.value) if user_info else \"Unknown user\" return str(user_info. value) if user_info else \"Unknown user\"  agent = create_react_agent( agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", model =\"anthropic:claude-3-7-sonnet-latest\", tools=[get_user_info], tools =[get_user_info], store=store # (8)!  store=store # (8)! store = store# (8)!) )  # Run the agent # Run the agentagent.invoke( agent. invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]}, {\"messages\":[{\"role\": \"user\", \"content\": \"look up user information\"}]}, config={\"configurable\": {\"user_id\": \"user_123\"}}  config={\"configurable\": {\"user_id\": \"user_123\"}} config ={\"configurable\":{\"user_id\": \"user_123\"}}) )\\n```\\n\\n1. The `InMemoryStore` is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the [store documentation][../reference/store.md) for more options. If you\\'re deploying with **LangGraph Platform**, the platform will provide a production-ready store for you.\\n2. For this example, we write some sample data to the store using the `put` method. Please see the [BaseStore.put](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.put) API reference for more details.\\n3. The first argument is the namespace. This is used to group related data together. In this case, we are using the `users` namespace to group user data.\\n4. A key within the namespace. This example uses a user ID for the key.\\n5. The data that we want to store for the given user.\\n6. The `get_store` function is used to access the store. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created.\\n7. The `get` method is used to retrieve data from the store. The first argument is the namespace, and the second argument is the key. This will return a `StoreValue` object, which contains the value and metadata about the value.\\n8. The `store` is passed to the agent. This enables the agent to access the store when running tools. You can also use the `get_store` function to access the store from anywhere in your code.\\n\\nTo **update** information in the store:\\n\\n*API Reference: [RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [get\\\\_store](https://langchain-ai.github.io/langgraph/reference/config/#langgraph.config.get_store)*\\n\\n```\\nfrom langchain_core.runnables import RunnableConfig from  langchain_core.runnables  import RunnableConfigfrom langchain_core.tools import tool from  langchain_core.tools  import toolfrom langgraph.graph import StateGraph from  langgraph.graph  import StateGraphfrom langgraph.config import get_store from langgraph.config import get_store from  langgraph.config  import get_store  @tool @tooldef save_user_info(user_info: str, config: RunnableConfig) -> str: def  save_user_info(user_info: str, config: RunnableConfig) -> str: \"\"\"Save user info.\"\"\"  \"\"\"Save user info.\"\"\" # Same as that provided to `builder.compile(store=store)` # Same as that provided to `builder.compile(store=store)`  # or `create_react_agent` # or `create_react_agent` store = get_store()  store = get_store() store = get_store() user_id = config[\"configurable\"].get(\"user_id\") user_id = config[\"configurable\"]. get(\"user_id\") store.put((\"users\",), user_id, user_info)  store.put((\"users\",), user_id, user_info) store. put((\"users\",), user_id, user_info) return \"Successfully saved user info.\" return\"Successfully saved user info.\"  builder = StateGraph(...) builder = StateGraph(...)... ...graph = builder.compile(store=store) graph = builder. compile(store = store)\\n```\\n\\n Update long-term memory\\n\\n```\\nfrom typing_extensions import TypedDict from  typing_extensions  import TypedDict  from langchain_core.tools import tool from  langchain_core.tools  import toolfrom langgraph.config import get_store from  langgraph.config  import get_storefrom langchain_core.runnables import RunnableConfig from  langchain_core.runnables  import RunnableConfigfrom langgraph.prebuilt import create_react_agent from  langgraph.prebuilt  import create_react_agentfrom langgraph.store.memory import InMemoryStore from  langgraph.store.memory  import InMemoryStore  store = InMemoryStore() # (1)! store = InMemoryStore()# (1)!  class UserInfo(TypedDict): # (2)! class  UserInfo(TypedDict):# (2)! name: str name: str  @tool @tooldef save_user_info(user_info: UserInfo, config: RunnableConfig) -> str: # (3)! def  save_user_info(user_info: UserInfo, config: RunnableConfig) -> str:# (3)! \"\"\"Save user info.\"\"\"  \"\"\"Save user info.\"\"\"  # Same as that provided to `create_react_agent` # Same as that provided to `create_react_agent` store = get_store() # (4)!  store = get_store() # (4)! store = get_store()# (4)! user_id = config[\"configurable\"].get(\"user_id\") user_id = config[\"configurable\"]. get(\"user_id\") store.put((\"users\",), user_id, user_info) # (5)!  store.put((\"users\",), user_id, user_info) # (5)! store. put((\"users\",), user_id, user_info)# (5)! return \"Successfully saved user info.\" return\"Successfully saved user info.\"  agent = create_react_agent( agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", model =\"anthropic:claude-3-7-sonnet-latest\", tools=[save_user_info], tools =[save_user_info], store=store  store=store store = store) )  # Run the agent # Run the agentagent.invoke( agent. invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"My name is John Smith\"}]}, {\"messages\":[{\"role\": \"user\", \"content\": \"My name is John Smith\"}]}, config={\"configurable\": {\"user_id\": \"user_123\"}} # (6)!  config={\"configurable\": {\"user_id\": \"user_123\"}} # (6)! config ={\"configurable\":{\"user_id\": \"user_123\"}}# (6)!) )  # You can access the store directly to get the value # You can access the store directly to get the valuestore.get((\"users\",), \"user_123\").value store. get((\"users\",), \"user_123\"). value\\n```\\n\\n1. The `InMemoryStore` is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the [store documentation](../../reference/store/) for more options. If you\\'re deploying with **LangGraph Platform**, the platform will provide a production-ready store for you.\\n2. The `UserInfo` class is a `TypedDict` that defines the structure of the user information. The LLM will use this to format the response according to the schema.\\n3. The `save_user_info` function is a tool that allows an agent to update user information. This could be useful for a chat application where the user wants to update their profile information.\\n4. The `get_store` function is used to access the store. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created.\\n5. The `put` method is used to store data in the store. The first argument is the namespace, and the second argument is the key. This will store the user information in the store.\\n6. The `user_id` is passed in the config. This is used to identify the user whose information is being updated.\\n\\n## Advanced tool features[¶](#advanced-tool-features \"Permanent link\")\\n\\n### Immediate return[¶](#immediate-return \"Permanent link\")\\n\\nUse `return_direct=True` to immediately return a tool\\'s result without executing additional logic.\\n\\nThis is useful for tools that should not trigger further processing or tool calls, allowing you to return results directly to the user.\\n\\n```\\n@tool(return_direct=True) @tool(return_direct=True) @tool(return_direct = True)def add(a: int, b: int) -> int: def  add(a: int, b: int) -> int:  \"\"\"Add two numbers\"\"\"  \"\"\"Add two numbers\"\"\" return a + b return a + b\\n```\\n\\n Extended example: Using return\\\\_direct in a prebuilt agent\\n\\n```\\nfrom langchain_core.tools import tool from  langchain_core.tools  import toolfrom langgraph.prebuilt import create_react_agent from  langgraph.prebuilt  import create_react_agent  @tool(return_direct=True) @tool(return_direct=True) @tool(return_direct = True)def add(a: int, b: int) -> int: def  add(a: int, b: int) -> int:  \"\"\"Add two numbers\"\"\"  \"\"\"Add two numbers\"\"\" return a + b return a + b  agent = create_react_agent( agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", model =\"anthropic:claude-3-7-sonnet-latest\", tools=[add] tools =[add]) )  agent.invoke( agent. invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what\\'s 3 + 5?\"}]} {\"messages\":[{\"role\": \"user\", \"content\":\"what\\'s 3 + 5?\"}]}) )\\n```\\n\\nUsing without prebuilt components\\n\\nIf you are building a custom workflow and are not relying on `create_react_agent` or `ToolNode`, you will also need to implement the control flow to handle `return_direct=True`.\\n\\n### Force tool use[¶](#force-tool-use \"Permanent link\")\\n\\nIf you need to force a specific tool to be used, you will need to configure this at the **model** level using the `tool_choice` parameter in the bind\\\\_tools method.\\n\\nForce specific tool usage via tool\\\\_choice:\\n\\n```\\n@tool(return_direct=True) @tool(return_direct = True)def greet(user_name: str) -> int: def  greet(user_name: str) -> int: \"\"\"Greet user.\"\"\"  \"\"\"Greet user.\"\"\" return f\"Hello {user_name}!\" return f \"Hello {user_name}!\"  tools = [greet] tools =[greet]  configured_model = model.bind_tools( configured_model = model. bind_tools( tools, tools,  # Force the use of the \\'greet\\' tool # Force the use of the \\'greet\\' tool tool_choice={\"type\": \"tool\", \"name\": \"greet\"}  tool_choice={\"type\": \"tool\", \"name\": \"greet\"} tool_choice ={\"type\": \"tool\", \"name\": \"greet\"}) )\\n```\\n\\n Extended example: Force tool usage in an agent\\n\\nTo force the agent to use specific tools, you can set the `tool_choice` option in `model.bind_tools()`:\\n\\n```\\nfrom langchain_core.tools import tool from  langchain_core.tools  import tool  @tool(return_direct=True) @tool(return_direct=True) @tool(return_direct = True)def greet(user_name: str) -> int: def  greet(user_name: str) -> int: \"\"\"Greet user.\"\"\"  \"\"\"Greet user.\"\"\" return f\"Hello {user_name}!\" return f \"Hello {user_name}!\"  tools = [greet] tools =[greet]  agent = create_react_agent( agent = create_react_agent( model=model.bind_tools(tools, tool_choice={\"type\": \"tool\", \"name\": \"greet\"}),  model=model.bind_tools(tools, tool_choice={\"type\": \"tool\", \"name\": \"greet\"}), model = model. bind_tools(tools, tool_choice ={\"type\": \"tool\", \"name\": \"greet\"}), tools=tools tools = tools) )  agent.invoke( agent. invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"Hi, I am Bob\"}]} {\"messages\":[{\"role\": \"user\", \"content\":\"Hi, I am Bob\"}]}) )\\n```\\n\\nAvoid infinite loops\\n\\nForcing tool usage without stopping conditions can create infinite loops. Use one of the following safeguards:\\n\\n* Mark the tool with [`return_direct=True`](#immediate-return) to end the loop after execution.\\n* Set [`recursion_limit`](../../concepts/low_level/#recursion-limit) to restrict the number of execution steps.\\n\\nTool choice configuration\\n\\nThe `tool_choice` parameter is used to configure which tool should be used by the model when it decides to call a tool. This is useful when you want to ensure that a specific tool is always called for a particular task or when you want to override the model\\'s default behavior of choosing a tool based on its internal logic.\\n\\nNote that not all models support this feature, and the exact configuration may vary depending on the model you are using.\\n\\n### Disable parallel calls[¶](#disable-parallel-calls \"Permanent link\")\\n\\nFor supported providers, you can disable parallel tool calling by setting `parallel_tool_calls=False` via the `model.bind_tools()` method:\\n\\n```\\nmodel.bind_tools( model. bind_tools( tools, tools, parallel_tool_calls=False  parallel_tool_calls=False parallel_tool_calls = False) )\\n```\\n\\n Extended example: disable parallel tool calls in a prebuilt agent\\n\\n```\\nfrom langchain.chat_models import init_chat_model from  langchain.chat_models  import init_chat_model  def add(a: int, b: int) -> int: def  add(a: int, b: int) -> int:  \"\"\"Add two numbers\"\"\"  \"\"\"Add two numbers\"\"\" return a + b return a + b  def multiply(a: int, b: int) -> int: def  multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\"  \"\"\"Multiply two numbers.\"\"\" return a * b return a* b  model = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\", temperature=0) model = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\", temperature = 0)tools = [add, multiply] tools =[add, multiply]agent = create_react_agent( agent = create_react_agent( # disable parallel tool calls # disable parallel tool calls model=model.bind_tools(tools, parallel_tool_calls=False),  model=model.bind_tools(tools, parallel_tool_calls=False), model = model. bind_tools(tools, parallel_tool_calls = False), tools=tools tools = tools) )  agent.invoke( agent. invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what\\'s 3 + 5 and 4 * 7?\"}]} {\"messages\":[{\"role\": \"user\", \"content\":\"what\\'s 3 + 5 and 4 * 7?\"}]}) )\\n```\\n\\n### Handle errors[¶](#handle-errors \"Permanent link\")\\n\\nLangGraph provides built-in error handling for tool execution through the prebuilt [ToolNode](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.tool_node.ToolNode) component, used both independently and in prebuilt agents.\\n\\nBy **default**, `ToolNode` catches exceptions raised during tool execution and returns them as `ToolMessage` objects with a status indicating an error.\\n\\n*API Reference: [AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)*\\n\\n```\\nfrom langchain_core.messages import AIMessage from  langchain_core.messages  import AIMessagefrom langgraph.prebuilt import ToolNode from  langgraph.prebuilt  import ToolNode  def multiply(a: int, b: int) -> int: def  multiply(a: int, b: int) -> int: if a == 42: if a == 42: raise ValueError(\"The ultimate error\") raise ValueError(\"The ultimate error\") return a * b return a* b  # Default error handling (enabled by default) # Default error handling (enabled by default)tool_node = ToolNode([multiply]) tool_node = ToolNode([multiply])  message = AIMessage( message = AIMessage( content=\"\", content = \"\", tool_calls=[{ tool_calls =[{ \"name\": \"multiply\", \"name\": \"multiply\", \"args\": {\"a\": 42, \"b\": 7}, \"args\":{\"a\": 42, \"b\": 7}, \"id\": \"tool_call_id\", \"id\": \"tool_call_id\", \"type\": \"tool_call\" \"type\": \"tool_call\" }] }]) )  result = tool_node.invoke({\"messages\": [message]}) result = tool_node. invoke({\"messages\":[message]})\\n```\\n\\nOutput:\\n\\n```\\n{\\'messages\\': [ {\\'messages\\': [ ToolMessage(  ToolMessage( content=\"Error: ValueError(\\'The ultimate error\\')\\\\n Please fix your mistakes.\",  content=\"Error: ValueError(\\'The ultimate error\\')\\\\n Please fix your mistakes.\", name=\\'multiply\\',  name=\\'multiply\\', tool_call_id=\\'tool_call_id\\',  tool_call_id=\\'tool_call_id\\', status=\\'error\\'  status=\\'error\\' )  )]} ]}\\n```\\n\\n#### Disable error handling[¶](#disable-error-handling \"Permanent link\")\\n\\nTo propagate exceptions directly, disable error handling:\\n\\n```\\ntool_node = ToolNode([multiply], handle_tool_errors=False) tool_node = ToolNode([multiply], handle_tool_errors = False)\\n```\\n\\nWith error handling disabled, exceptions raised by tools will propagate up, requiring explicit management.\\n\\n#### Custom error messages[¶](#custom-error-messages \"Permanent link\")\\n\\nProvide a custom error message by setting the error handling parameter to a string:\\n\\n```\\ntool_node = ToolNode( tool_node = ToolNode( [multiply], [multiply], handle_tool_errors=\"Can\\'t use 42 as the first operand, please switch operands!\" handle_tool_errors =\"Can\\'t use 42 as the first operand, please switch operands!\") )\\n```\\n\\nExample output:\\n\\n```\\n{\\'messages\\': [ {\\'messages\\':[ ToolMessage( ToolMessage( content=\"Can\\'t use 42 as the first operand, please switch operands!\", content =\"Can\\'t use 42 as the first operand, please switch operands!\", name=\\'multiply\\', name = \\'multiply\\', tool_call_id=\\'tool_call_id\\', tool_call_id = \\'tool_call_id\\', status=\\'error\\' status = \\'error\\' ) )]} ]}\\n```\\n\\n#### Error handling in agents[¶](#error-handling-in-agents \"Permanent link\")\\n\\nError handling in prebuilt agents (`create_react_agent`) leverages `ToolNode`:\\n\\n*API Reference: [create\\\\_react\\\\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)*\\n\\n```\\nfrom langgraph.prebuilt import create_react_agent from  langgraph.prebuilt  import create_react_agent  agent = create_react_agent( agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", model =\"anthropic:claude-3-7-sonnet-latest\", tools=[multiply] tools =[multiply]) )  # Default error handling # Default error handlingagent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what\\'s 42 x 7?\"}]}) agent. invoke({\"messages\":[{\"role\": \"user\", \"content\": \"what\\'s 42 x 7?\"}]})\\n```\\n\\nTo disable or customize error handling in prebuilt agents, explicitly pass a configured `ToolNode`:\\n\\n```\\ncustom_tool_node = ToolNode( custom_tool_node = ToolNode( [multiply], [multiply], handle_tool_errors=\"Cannot use 42 as a first operand!\" handle_tool_errors =\"Cannot use 42 as a first operand!\") )  agent_custom = create_react_agent( agent_custom = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", model =\"anthropic:claude-3-7-sonnet-latest\", tools=custom_tool_node tools = custom_tool_node) )  agent_custom.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what\\'s 42 x 7?\"}]}) agent_custom. invoke({\"messages\":[{\"role\": \"user\", \"content\": \"what\\'s 42 x 7?\"}]})\\n```\\n\\n### Handle large numbers of tools[¶](#handle-large-numbers-of-tools \"Permanent link\")\\n\\nAs the number of available tools grows, you may want to limit the scope of the LLM\\'s selection, to decrease token consumption and to help manage sources of error in LLM reasoning.\\n\\nTo address this, you can dynamically adjust the tools available to a model by retrieving relevant tools at runtime using semantic search.\\n\\nSee [`langgraph-bigtool`](https://github.com/langchain-ai/langgraph-bigtool) prebuilt library for a ready-to-use implementation.\\n\\n## Prebuilt tools[¶](#prebuilt-tools \"Permanent link\")\\n\\n### LLM provider tools[¶](#llm-provider-tools \"Permanent link\")\\n\\nYou can use prebuilt tools from model providers by passing a dictionary with tool specs to the `tools` parameter of `create_react_agent`. For example, to use the `web_search_preview` tool from OpenAI:\\n\\n*API Reference: [create\\\\_react\\\\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)*\\n\\n```\\nfrom langgraph.prebuilt import create_react_agent from  langgraph.prebuilt  import create_react_agent  agent = create_react_agent( agent = create_react_agent( model=\"openai:gpt-4o-mini\", model =\"openai:gpt-4o-mini\", tools=[{\"type\": \"web_search_preview\"}] tools =[{\"type\": \"web_search_preview\"}]) )response = agent.invoke( response = agent. invoke( {\"messages\": [\"What was a positive news story from today?\"]} {\"messages\":[\"What was a positive news story from today?\"]}) )\\n```\\n\\nPlease consult the documentation for the specific model you are using to see which tools are available and how to use them.\\n\\n### LangChain tools[¶](#langchain-tools \"Permanent link\")\\n\\nAdditionally, LangChain supports a wide range of prebuilt tool integrations for interacting with APIs, databases, file systems, web data, and more. These tools extend the functionality of agents and enable rapid development.\\n\\nYou can browse the full list of available integrations in the [LangChain integrations directory](https://python.langchain.com/docs/integrations/tools/).\\n\\nSome commonly used tool categories include:\\n\\n* **Search**: Bing, SerpAPI, Tavily\\n* **Code interpreters**: Python REPL, Node.js REPL\\n* **Databases**: SQL, MongoDB, Redis\\n* **Web data**: Web scraping and browsing\\n* **APIs**: OpenWeatherMap, NewsAPI, and others\\n\\nThese integrations can be configured and added to your agents using the same `tools` parameter shown in the examples above.\\n\\n '},\n",
       " {'title': 'autogen_ext.tools.langchain — AutoGen',\n",
       "  'url': 'https://microsoft.github.io/autogen/stable//reference/python/autogen_ext.tools.langchain.html',\n",
       "  'content': '* GitHub * GitHub # autogen\\\\_ext.tools.langchain# Allows you to wrap a LangChain tool and make it available to AutoGen. This class requires the `langchain` extra for the `autogen-ext` package. import  asyncio import  pandas  as  pd from  langchain_experimental.tools.python.tool  import PythonAstREPLTool from  autogen_ext.tools.langchain  import LangChainToolAdapter from  autogen_ext.models.openai  import OpenAIChatCompletionClient from  autogen_agentchat.messages  import TextMessage from  autogen_agentchat.agents  import AssistantAgent from  autogen_agentchat.ui  import Console from  autogen_core  import CancellationToken async def  main() -> None: df = pd. on_messages_stream([TextMessage(content = \"What\\'s the average age of the passengers?\", source = \"user\")], CancellationToken())) asyncio. run(main()) llm = ChatOpenAI(temperature = 0) toolkit = SQLDatabaseToolkit(db = db, llm = llm)# Create the LangChain tool adapter for every tool in the toolkit. run(main()) *async* run(*args: BaseModel*, *cancellation\\\\_token: CancellationToken*) → Any\")# Edit on GitHub',\n",
       "  'score': 0.97805,\n",
       "  'raw_content': '* [GitHub](https://github.com/microsoft/autogen \"GitHub\")\\n* [Discord](https://aka.ms/autogen-discord \"Discord\")\\n* [Twitter](https://twitter.com/pyautogen \"Twitter\")\\n\\n* [GitHub](https://github.com/microsoft/autogen \"GitHub\")\\n* [Discord](https://aka.ms/autogen-discord \"Discord\")\\n* [Twitter](https://twitter.com/pyautogen \"Twitter\")\\n\\n# autogen\\\\_ext.tools.langchain[#](#module-autogen_ext.tools.langchain \"Link to this heading\")\\n\\n*class* LangChainToolAdapter(*langchain\\\\_tool: LangChainTool*)[[source]](../../_modules/autogen_ext/tools/langchain/_langchain_adapter.html#LangChainToolAdapter)[#](#autogen_ext.tools.langchain.LangChainToolAdapter \"Link to this definition\")\\n:   Bases: [`BaseTool`](autogen_core.tools.html#autogen_core.tools.BaseTool \"autogen_core.tools._base.BaseTool\")[`BaseModel`, [`Any`](https://docs.python.org/3/library/typing.html#typing.Any \"(in Python v3.13)\")]\\n\\n    Allows you to wrap a LangChain tool and make it available to AutoGen.\\n\\n    Note\\n\\n    This class requires the `langchain` extra for the `autogen-ext` package.\\n\\n    ```\\n       \"autogen-ext[langchain]\"\\n    ```\\n\\n    Parameters:\\n    :   **langchain\\\\_tool** (*LangChainTool*) – A LangChain tool to wrap\\n\\n    Examples\\n\\n    Use the PythonAstREPLTool from the langchain\\\\_experimental package to create a tool that allows you to interact with a Pandas DataFrame.\\n\\n    ```\\n    import  asyncio import  pandas  as  pd from  langchain_experimental.tools.python.tool  import PythonAstREPLTool from  autogen_ext.tools.langchain  import LangChainToolAdapter from  autogen_ext.models.openai  import OpenAIChatCompletionClient from  autogen_agentchat.messages  import TextMessage from  autogen_agentchat.agents  import AssistantAgent from  autogen_agentchat.ui  import Console from  autogen_core  import CancellationToken async def  main() -> None: df = pd. read_csv(\"https://raw.githubusercontent.com/pandas-dev/pandas/main/doc/data/titanic.csv\")# type: ignore tool = LangChainToolAdapter(PythonAstREPLTool(locals ={\"df\": df})) model_client = OpenAIChatCompletionClient(model =\"gpt-4o\") agent = AssistantAgent(\"assistant\", tools =[tool], model_client = model_client, system_message =\"Use the `df` variable to access the dataset.\",) await Console(agent. on_messages_stream([TextMessage(content = \"What\\'s the average age of the passengers?\", source = \"user\")], CancellationToken())) asyncio. run(main())\\n    ```\\n\\n    This example demonstrates how to use the SQLDatabaseToolkit from the langchain\\\\_community package to interact with an SQLite database. It uses the `RoundRobinGroupChat` to iterate the single agent over multiple steps. If you want to one step at a time, you can just call run\\\\_stream method of the [`AssistantAgent`](autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent \"autogen_agentchat.agents.AssistantAgent\") class directly.\\n\\n    ```\\n    import  asyncio import  sqlite3 import  requests from  autogen_agentchat.agents  import AssistantAgent from  autogen_agentchat.conditions  import TextMentionTermination from  autogen_agentchat.teams  import RoundRobinGroupChat from  autogen_agentchat.ui  import Console from  autogen_ext.models.openai  import OpenAIChatCompletionClient from  autogen_ext.tools.langchain  import LangChainToolAdapter from  langchain_community.agent_toolkits.sql.toolkit  import SQLDatabaseToolkit from  langchain_community.utilities.sql_database  import SQLDatabase from  langchain_openai  import ChatOpenAI from  sqlalchemy  import Engine, create_engine from  sqlalchemy.pool  import StaticPool def  get_engine_for_chinook_db() -> Engine: url =\"https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql\" response = requests. get(url) sql_script = response. text connection = sqlite3. connect(\":memory:\", check_same_thread = False) connection. executescript(sql_script) return create_engine(\"sqlite://\", creator = lambda: connection, poolclass = StaticPool, connect_args ={\"check_same_thread\": False},) async def  main() -> None:# Create the engine and database wrapper. engine = get_engine_for_chinook_db() db = SQLDatabase(engine)# Create the toolkit. llm = ChatOpenAI(temperature = 0) toolkit = SQLDatabaseToolkit(db = db, llm = llm)# Create the LangChain tool adapter for every tool in the toolkit. tools =[LangChainToolAdapter(tool) for tool in toolkit. get_tools()]# Create the chat completion client. model_client = OpenAIChatCompletionClient(model =\"gpt-4o\")# Create the assistant agent. agent = AssistantAgent(\"assistant\", model_client = model_client, tools = tools,# type: ignore model_client_stream = True, system_message =\"Respond with \\'TERMINATE\\' if the task is completed.\",)# Create termination condition. termination = TextMentionTermination(\"TERMINATE\")# Create a round-robin group chat to iterate the single agent over multiple steps. chat = RoundRobinGroupChat([agent], termination_condition = termination)# Run the chat. await Console(chat. run_stream(task = \"Show some tables in the database\")) if __name__ == \"__main__\": asyncio. run(main())\\n    ```\\n\\n    *async* run(*args: BaseModel*, *cancellation\\\\_token: [CancellationToken](autogen_core.html#autogen_core.CancellationToken \"autogen_core._cancellation_token.CancellationToken\")*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any \"(in Python v3.13)\")[[source]](../../_modules/autogen_ext/tools/langchain/_langchain_adapter.html#LangChainToolAdapter.run)[#](#autogen_ext.tools.langchain.LangChainToolAdapter.run \"Link to this definition\")\\n\\nOn this page\\n\\n[Edit on GitHub](https://github.com/microsoft/autogen/edit/main/python/docs/src/reference/python/autogen_ext.tools.langchain.rst)\\n\\n[Show Source](../../_sources/reference/python/autogen_ext.tools.langchain.rst.txt)\\n\\n '},\n",
       " {'title': 'LangChain - Machine Learning Studies - Jerome Boyer',\n",
       "  'url': 'https://jbcodeforce.github.io/ML-studies/coding/langchain/',\n",
       "  'content': 'LangChain is a open-source framework for developing applications powered by large language models, connecting them to external data sources, and manage',\n",
       "  'score': 0.97439,\n",
       "  'raw_content': '[Skip to content](#langchain-study)\\n\\n\\n\\n* [Value propositions](#value-propositions) \\n\\n  + [Sources](#sources)\\n* [LangChain libraries](#langchain-libraries) \\n\\n  + [Getting started](#getting-started)\\n* [Main Concepts](#main-concepts) \\n\\n  + [Model I/O](#model-io)\\n  + [Chain](#chain)\\n  + [Runnable](#runnable)\\n  + [Memory](#memory)\\n  + [Retrieval Augmented Generation](#retrieval-augmented-generation)\\n  + [Q&A app](#qa-app)\\n  + [ChatBot](#chatbot)\\n  + [Text Generation Examples](#text-generation-examples)\\n  + [Summarization chain](#summarization-chain)\\n  + [Evaluating results](#evaluating-results)\\n* [Agent](#agent) \\n\\n  + [Tool Calling](#tool-calling)\\n  + [Interesting tools](#interesting-tools)\\n  + [How Tos](#how-tos)\\n* [LangChain Expression Language (LCEL)](#langchain-expression-language-lcel)\\n\\n# LangChain Study[¶](#langchain-study \"Permanent link\")\\n\\nIn LLM application there are a lot of steps to do, trying different prompting, integrating different LLMs, implementing conversation history, at the end there is a lot of glue code to implement.\\n\\n[LangChain](https://python.langchain.com/docs/get_started/introduction) is a open-source framework for developing applications powered by large language models, connecting them to external data sources, and manage conversation with human.\\n\\n## Value propositions[¶](#value-propositions \"Permanent link\")\\n\\nDevelop apps with context awareness, and that can reason using LLMs. It includes Python and Typescript packages, and a Java one under construction.\\n\\nIt focuses on composition and modularity. The components defined by the framework can be combined to address specific use cases, and developers can add new components.\\n\\n* **LangChain**: Python and Javascript libraries\\n* **LangServe:** a library for deploying LangChain chains as a REST API.\\n* **LangSmith:** a platform that lets developers debug, test, evaluate, and monitor chains\\n* Predefined prompt template from langChain Hub.\\n\\nThey are adding new products to their portfolio quickly like LangSmith (get visibility on LLMs execution), and LangServe (server API for LangChain apps).\\n\\n### Sources[¶](#sources \"Permanent link\")\\n\\nThe content comes from different sources:\\n\\n* [Excellent product documentation](https://python.langchain.com/docs/), should be the go to place.\\n* [deeplearning.ai LangChain introduction by Harisson Chase and Andrew Ng](https://learn.deeplearning.ai)\\n* [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/)\\n* [Retrieval and RAG blog.](https://blog.langchain.dev/retrieval/)\\n\\n## LangChain libraries[¶](#langchain-libraries \"Permanent link\")\\n\\nThe core building block of LangChain applications is the LLMChain:\\n\\n* A LLM\\n* Prompt templates\\n* Output parsers\\n\\n[PromptTemplate](https://python.langchain.com/docs/modules/model_io/prompts) helps to structure the prompt and facilitate reuse by creating model agnostic templates. The library includes output parsers to get content extracted from the keyword defined in the prompt. Example is the chain of thought keywords of **Thought, Action, Observation.**\\n\\n### Getting started[¶](#getting-started \"Permanent link\")\\n\\nThe [LangChain documentation](https://python.langchain.com/docs/get_started/quickstart/) is excellent so no need to write more. All my study codes with LangChain and LLM are in different folders of this repo:\\n\\n| Backend | Type of chains |\\n| --- | --- |\\n| [openAI](https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/openAI) | The implementation of the quickstart examples, RAG, chatbot, agent |\\n| [Ollama](https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/ollama) | run a simple query to Ollama (running Llama 3.2) locally |\\n| [Anthropic Claude](https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/anthropic) |\\n| [Mistral LLM](https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/mistral) |\\n| [IBM WatsonX](https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/watsonX) |\\n| [AWS Bedrock](https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/bedrock) | zero\\\\_shot generation |\\n\\nEach code needs to define only the needed LangChain modules to keep the executable size low.\\n\\n## Main Concepts[¶](#main-concepts \"Permanent link\")\\n\\n### Model I/O[¶](#model-io \"Permanent link\")\\n\\n[Model I/O](https://python.langchain.com/docs/modules/model_io/) are building blocks to interface with any language model. It facilitates the interface of model input (prompts) with the LLM model to produce the model output.\\n\\n* LangChain supports two types of language: LLM (for pure text completion models) or ChatModel (conversation on top of LLM using constructs of AIMessage, HumanMessage)\\n* LangChain uses [Prompt templates](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/) to control LLM behavior.\\n\\n  + Two common prompt templates: [string prompt](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.base.StringPromptTemplate.html) templates and [chat prompt](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html) templates.\\n\\n  ```\\n  from  langchain_core.prompts  import ChatPromptTemplate, MessagesPlaceholder prompt = ChatPromptTemplate. from_messages([(\"system\",\"Answer the user\\'s questions based on the below context: \\\\n\\\\n{context} \"), MessagesPlaceholder(variable_name = \"chat_history\"),(\"user\", \"{input} \"),])\\n  ```\\n\\n  \\\\* We can build custom prompt by extending existing default templates. An example is a \\'few-shot-examples\\' in a chat prompt using [FewShotChatMessagePromptTemplate](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/few_shot_examples_chat). \\\\* LangChain offers a [prompt hub](https://smith.langchain.com/hub) to get predefined prompts easily loadable:\\n\\n  ```\\n  from  langchain  import hub prompt = hub. pull(\"hwchase17/openai-functions-agent\")\\n  ```\\n* [Chains](https://python.langchain.com/docs/modules/chains/) allow developers to combine multiple components together (or to combine other chains) to create a single, coherent application.\\n* **OutputParsers** convert the raw output of a language model into a format that can be used downstream\\n\\n*Feature stores, like [Feast](https://github.com/feast-dev/feast), can be a great way to keep information about the user conversation or query, and LangChain provides an easy way to combine data from Feast with LLMs.*\\n\\n### Chain[¶](#chain \"Permanent link\")\\n\\nChains are [runnable](#runnable), observable and composable. The LangChain framework uses the Runnable class to encapsulate operations that can be run synchronously or asynchronously.\\n\\n* [LLMChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html) class is the basic chain to integrate with any LLM.\\n\\n  ```\\n  # Basic chain chain = LLMChain(llm = model, prompt = prompt) chain. invoke(\"a query\")\\n  ```\\n* Sequential chain combines chains in sequence with single input and output (SimpleSequentialChain)\\n\\n  ```\\n  overall_simple_chain = SimpleSequentialChain(chains =[chain_one, chain_two], verbose = True)\\n  ```\\n\\n  or multiple inputs and outputs with prompt using the different environment variables (see [this code](https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/openAI/multi_chains.py)).\\n* [LLMRouterChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.router.llm_router.LLMRouterChain.html#langchain.chains.router.llm_router.LLMRouterChain) is a chain that outputs the name of a destination chain and the inputs to it.\\n* [LangChain Expression Language](https://python.langchain.com/docs/expression_language/) is a declarative way to define chains. It looks similar to Unix shell pipe: input for one runnable comes from the output of predecessor (This is why prompt below is a runnable).\\n\\n  ```\\n  # a chain definition using Langchain expression language chain = prompt | model | output_parser\\n  ```\\n* Chain can be executed asynchronously in its own Thread using the `ainvoke` method.\\n\\n### Runnable[¶](#runnable \"Permanent link\")\\n\\n[Runnable interface](https://python.langchain.com/v0.1/docs/expression_language/interface/) is a protocol to define custom chains and invoke them. Each Runnable exposes methods to get input, output and config schemas. Each implements synchronous and async invoke methods and batch. Runnable can run in parallel or in sequence.\\n\\nTo pass data to a Runnable there is the `RunnablePassthrough` class. This is used in conjunction with RunnableParallel to assign data to key in a map.\\n\\n```\\nfrom  langchain.schema.runnable  import RunnableParallel, RunnablePassthrough runnable = RunnableParallel(passed = RunnablePassthrough(), extra = RunnablePassthrough. assign(mult = lambda x: x[\"num\"]* 3), modified = lambda x: x[\"num\"] + 1) print(runnable. invoke({\"num\": 6})){\\'passed\\':{\\'num\\': 6}, \\'extra\\':{\\'num\\': 6, \\'mult\\': 18}, \\'modified\\': 7}\\n```\\n\\n* [RunnableLambda](https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/runnables/base.py#L3572) is a type of Runnable that wraps a callable function.\\n\\n```\\nsequence = RunnableLambda(lambda x: x + 1) |{\\'mul_2\\': RunnableLambda(lambda x: x* 2), \\'mul_5\\': RunnableLambda(lambda x: x* 5)} sequence. invoke(1)\\n```\\n\\nThe [RunnablePassthrough.assign](https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/runnables/passthrough.py) method is used to create a Runnable that passes the input through while adding some keys to the output.\\n\\nWe can use `Runnable.bind()` to pass arguments as constants accessible within a runnable sequence (a chain) where argument is not part of the output of preceding runnables in the sequence.\\n\\nSee some code [RunnableExamples](https://github.com/jbcodeforce/ML-studies/tree/master/e2e-demos/ollama-mistral/RunnableExamples.py)\\n\\n### Memory[¶](#memory \"Permanent link\")\\n\\nLarge Language Models are stateless and do not remember anything. Chatbot seems to have memory, because conversation is kept in the context.\\n\\nWith a simple conversation like the following code, the conversation is added as string into the context:\\n\\n```\\nllm = ChatOpenAI(temperature = 0) memory = ConversationBufferMemory() conversation = ConversationChain(llm = llm, memory = memory, verbose = True # trace the chain)\\n```\\n\\nThe memory is just a container in which we can save {\"input:\"\"} and {\"output\": \"\"} content.\\n\\nBut as the conversation goes, the size of the context grows, and so the cost of operating this chatbot, as API are charged by the size of the token. Using `ConversationBufferWindowMemory(k=1)` with a k necessary to keep enough context, we can limit cost. Same with `ConversationTokenBufferMemory` to limit the token in memory.\\n\\n[ConversationChain](https://python.langchain.com/docs/modules/memory/conversational_customization/) is a predefined chain to have a conversation and load context from memory.\\n\\nAs part of memory component there is the [ConversationSummaryMemory](https://python.langchain.com/docs/modules/memory/types/summary/) to get the conversation summary so far.\\n\\nThe other important memory is [Vector Data memory](https://python.langchain.com/docs/modules/memory/types/vectorstore_retriever_memory/) and entity memory or [knowledgeGraph](https://python.langchain.com/docs/modules/memory/types/kg/)\\n\\nSee related code [conversation\\\\_with\\\\_memory.py](https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/openAI/conversation_with_memory.py)\\n\\n### Retrieval Augmented Generation[¶](#retrieval-augmented-generation \"Permanent link\")\\n\\nThe goal for **Retrieval Augmented Generation** (RAG) is to add custom dataset not already part of a trained model and use the dataset as input sent to the LLM. RAG is illustrated in figure below:\\n\\nEmbed is the vector representation of a chunk of text. Different embedding can be used.\\n\\n Embeddings\\n\\nThe classical Embedding is the [OpenAIEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.openai.OpenAIEmbeddings.htm) but Hugging Face offers an open source version: the **SentenceTransformers**https://huggingface.co/sentence-transformers which is a Python framework for state-of-the-art sentence, text and image embeddings.\\n\\n```\\nfrom  langchain_openai  import OpenAIEmbeddings vectorstore = Chroma. from_documents(documents = splits, embedding = OpenAIEmbeddings(), persist_directory = DOMAIN_VS_PATH) # With HuggingFace from  sentence_transformers  import SentenceTransformer def  build_embedding(docs): model = SentenceTransformer(\"all-MiniLM-L6-v2\") return model. encode(docs) # With AWS embedding from  langchain.embeddings  import BedrockEmbeddings\\n```\\n\\nDifferent code that implement RAG\\n\\n| Code | Notes |\\n| --- | --- |\\n| [build\\\\_agent\\\\_domain\\\\_rag.py](https://github.com/jbcodeforce/ML-studies/blob/master/RAG/build_agent_domain_rag.py/) | Read Lilian Weng blog and create a ChromeDB vector store with OpenAIEmbeddings |\\n| [query\\\\_agent\\\\_domain\\\\_store.py](https://github.com/jbcodeforce/ML-studies/blob/master/RAG/query_agent_domain_store.py/) | Query the persisted vector store for similarity search |\\n| [prepareVectorStore.py](https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/Q&A/prepareVectorStore.py) | Use AWS Bedrock Embeddings |\\n| [embeddings\\\\_hf.py](https://github.com/jbcodeforce/ML-studies/blob/master/RAG/embeddings_hf.py) | Use Hunggingface embeddings with splitting a markdown file and use FAISS vector store |\\n| [rag\\\\_HyDE.py](https://github.com/jbcodeforce/ML-studies/blob/master/RAG/rag_HyDE.py) | Hypothetical Document Embedding (HyDE) the first prompt create an hypothetical document |\\n\\nCreating chunks is necessary because language models generally have a limit to the amount of token they can deal with. It also improve the similarity search based on vector.\\n\\n Split docs and save in vector store\\n\\n[RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) splits text by recursively look at characters. `text_splitter.split_documents(documents)` return a list of [Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html) which is a wrapper to page content and some metadata for the indexes from the source document.\\n\\n```\\n# ... from  langchain.text_splitter  import RecursiveCharacterTextSplitter from  langchain.vectorstores  import FAISS from  langchain.indexes.vectorstore  import VectorStoreIndexWrapper loader = PyPDFDirectoryLoader(\"./data/\") documents = loader. load() text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100,) docs = text_splitter. split_documents(documents) vectorstore_faiss = FAISS. from_documents(docs, embeddings,) vectorstore_faiss. save_local(\"faiss_index\")\\n```\\n\\n  Search similarity in vector DB\\n\\n[OpenAIEmbeddings](https://python.langchain.com/docs/integrations/text_embedding/openai/)\\n\\n```\\nembeddings = OpenAIEmbeddings(model =\"text-embedding-3-large\", dimensions = 1024) query =\"\"\"Is it possible that ...?\"\"\" query_embedding = embeddings. embed_query(query) relevant_documents = vectorstore_faiss. similarity_search_by_vector(query_embedding)\\n```\\n\\nDuring the interaction with the end-user, the system (a chain in LangChain) retrieves the most relevant data to the question asked, and passes it to LLM in the generation step.\\n\\n* Embeddings capture the semantic meaning of the text to help do similarity search\\n* Persist the embeddings into a Vector store. Faiss and ChromaDB are common vector stores to use, but OpenSearch, Postgresql can also being used.\\n* Retriever includes semantic search and efficient algorithm to prepare the prompt. To improve on vector similarity search we can generate variants of the input question.\\n\\nSee [Q&A with FAISS store qa-faiss-store.py](https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/Q&A/qa-faiss-store.py).\\n\\n* [Another example of LLM Chain with AWS Bedrock llm and Feast as feature store](https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/feast/feast-prompt.py)\\n* **[Web scraping](https://python.langchain.com/docs/use_cases/web_scraping)** for LLM based web research. It uses the same process: document/page loading, transformation with tool like BeautifulSoup, to HTML2Text.\\n\\n Getting started with Feast\\n\\nUse `pip install feast` then the `feast` CLI with `feast init my_feature_repo` to create a Feature Store then `feast apply` to create entity, feature views, and services. Then `feast ui` + <http://localhost:8888> to act on the store.\\n\\n  LLM and FeatureForm\\n\\nSee [FeatureForm](https://docs.featureform.com/) as another open-source feature store solution and the LangChain sample with [Claude LLM](https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/featureform/ff-langchain-prompt.py)\\n\\n### Q&A app[¶](#qa-app \"Permanent link\")\\n\\nFor **Q&A** the pipeline will most likely integrate with existing documents as illustrated in the figure below:\\n\\n**Embeddings** capture the semantic meaning of the text, which helps to do similarity search. **Vector store** supports storage and searching of these embeddings. Retrievers use [different algorithms](https://python.langchain.com/docs/modules/data_connection/retrievers/) for the semantic search to load vectors.\\n\\n Use RAG with Q&A\\n\\nchains.RetrievalQA\\n\\n```\\nfrom  langchain.chains  import RetrievalQA from  langchain.prompts  import PromptTemplate prompt_template =\"\"\"Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.{context}Question: {question}Assistant:\"\"\" PROMPT = PromptTemplate(template = prompt_template, input_variables =[\"context\", \"question\"]) qa = RetrievalQA. from_chain_type(llm = llm, chain_type = \"stuff\", retriever = vectorstore_faiss. as_retriever(search_type = \"similarity\", search_kwargs ={\"k\": 3}), return_source_documents = True, chain_type_kwargs ={\"prompt\": PROMPT}) query = \"Is it possible that I get sentenced to jail due to failure in filings?\" result = qa({\"query\": query}) print_ww(result[\\'result\\'])\\n```\\n\\n### ChatBot[¶](#chatbot \"Permanent link\")\\n\\n**[Chatbots](https://python.langchain.com/docs/use_cases/chatbots/)** is the most common app for LLM: Aside from basic prompting and LLMs call, chatbots have **memory** and retrievers:\\n\\n### Text Generation Examples[¶](#text-generation-examples \"Permanent link\")\\n\\n* [Basic query with unknown content to generate hallucination: 1st\\\\_openAI\\\\_lc.py](ttps://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/openAI/1st_openAI_lc.py)\\n* [Simple test to call Bedrock with Langchain](https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/bedrock/TestBedrockWithLangchain.py) using on zero\\\\_shot generation.\\n* Response to an email of unhappy customer using Claude 2 and PromptTemplate. `PromptTemplates` allow us to create generic shells which can be populated with information and get model outputs based on different scenarios. See the [text\\\\_generation/ResponseToUnhappyCustomer.py code.](https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/text_generation/ResponseToUnhappyCustomer.py)\\n\\n### Summarization chain[¶](#summarization-chain \"Permanent link\")\\n\\nAlways assess the size of the content to send, as the approach can be different: for big document, we need to split the doc in chunks.\\n\\n* Small text summary with OpenAI.\\n* Small text to summarize, with [bedrock client](https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/bedrock/utils/bedrock.py) and the invoke\\\\_model on the client see the code in [llm-langchain/summarization/SmallTextSummarization.py](https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/summarization/SmallTextSummarization.py)\\n* For big document, langchain provides the load\\\\_summarize\\\\_chain to summarize by chunks and get the summary of the summaries. See code with \\'manual\\' extraction of the summaries as insights and then creating a summary of insights in [summarization/long-text-summarization.py](https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/bedrock/summarization/long-text-summarization.py) or using a LangChain summarization with map-reduce in [summarization/long-text-summarization-mr.py](https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/bedrock/summarization/long-text-summarization-mr.py).\\n\\n Using langchain summarize chain\\n\\n```\\nfrom  langchain.text_splitter  import RecursiveCharacterTextSplitter from  langchain.llms.bedrock  import Bedrock from  langchain.chains.summarize  import load_summarize_chain llm = Bedrock(model_id = modelId, model_kwargs ={\"max_tokens_to_sample\": 1000,}, client = boto3_bedrock,) text_splitter = RecursiveCharacterTextSplitter(separators =[\" \\\\n\\\\n \", \" \\\\n \"], chunk_size = 4000, chunk_overlap = 100) docs = text_splitter. create_documents([letter]) summary_chain = load_summarize_chain(llm = llm, chain_type = \"map_reduce\", verbose = True) output = summary_chain. run(docs)\\n```\\n\\n### Evaluating results[¶](#evaluating-results \"Permanent link\")\\n\\nThe evaluation of a chain, we need to define the data points to be measured. Building Questions and accepted answers is a classical approach.\\n\\nWe can use LLM and a special chain ([QAGenerateChain](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.qa.generate_chain.QAGenerateChain.html)) to build Q&A from a document.\\n\\n## Agent[¶](#agent \"Permanent link\")\\n\\n[Agent](https://python.langchain.com/v0.2/docs/concepts/#agents) is an orchestrator pattern where the LLM decides what actions to take from the current query and context. With chain, developer code the sequence of tasks, with agent the LLM decides. [LangGraph](../langgraph/) is an extension of LangChain specifically aimed at creating highly controllable and customizable agents.\\n\\n**Chains** let create a pre-defined sequence of tool usage(s), while **Agents** let the model uses tools in a loop, so that it can decide how many times to use its defined tools.\\n\\n AgentExecutor is deprecated\\n\\nUse LangGraph to implement agent.\\n\\nThis content is then from v0.1\\n\\nThere are [different types](https://python.langchain.com/docs/modules/agents/agent_types/) of agent: Intended Model, Supports Chat, Supports Multi-Input Tools, Supports Parallel Function Calling, Required Model Params.\\n\\nLangChain uses a specific [Schema model](https://python.langchain.com/docs/modules/agents/concepts/#schema) to define: **AgentAction**, with tool and tool\\\\_input and **AgentFinish**.\\n\\n```\\nfrom  langchain.agents  import create_tool_calling_agent from  langchain.agents  import AgentExecutor from  langchain.tools.retriever  import create_retriever_tool from  langchain_community.tools.tavily_search  import TavilySearchResults... tools =[retriever_tool, search, llm_math, wikipedia] agent = create_tool_calling_agent(llm, tools, prompt) agent_executor = AgentExecutor(agent = agent, tools = tools, verbose = True)\\n```\\n\\n* To create agents use one of the constructor methods such as: `create_react_agent, create_json_agent, create_structured_chat_agent`, [create\\\\_tool\\\\_calling\\\\_agent](https://api.python.langchain.com/en/latest/agents/langchain.agents.tool_calling_agent.base.create_tool_calling_agent.html#langchain-agents-tool-calling-agent-base-create-tool-calling-agent) etc. Those methods return a Runnable.\\n* The **Agent** loops on user input until it returns `AgentFinish` action. If the Agent returns an `AgentAction`, then use that to call a tool and get an `Observation`. Agent has input and output and `intermediate steps`. AgentAction is a response that consists of action and action\\\\_input.\\n* See the existing predefined [agent types](https://python.langchain.com/docs/modules/agents/agent_types/).\\n* [AgentExecutor](https://api.python.langchain.com/en/latest/agents/langchain.agents.agent.AgentExecutor.html) is the runtime for an agent.\\n* **Tools** are functions that an agent can invoke. It defines the input schema for the tool and the function to run. Parameters of the tool should be sensibly named and described.\\n\\n### Tool Calling[¶](#tool-calling \"Permanent link\")\\n\\nWith Tool Calling we can define function or tool to be referenced as part of the LLM response, and LLM will prepare the arguments for the function. It is used to generate tool invocations, not to execute it.\\n\\nTool calling allows a model to detect when one or more tools should be called and responds with the inputs that should be passed to those tools. The inputs match a defined schema. Below is an example structured answer from OpenAI LLM: \"tool\\\\_calls\" is the key to get the list of function names and arguments the orchestrator needs to call.\\n\\n```\\n \"tool_calls\":  [ { \"name\":  \"tavily_search_results_json\",  \"args\":  { \"query\":  \"weather in San Francisco\"  },  \"id\":  \"call_Vg6JRaaz8d06OXbG5Gv7Ea5J\"  }\\n```\\n\\nPrompt defines placeholders to get tools parameters. The following [langchain prompt](https://smith.langchain.com/hub/hwchase17/openai-tools-agent) for OpenAI uses `agent_scratchpad` variable, which is a `MessagesPlaceholder`. Intermediate agent actions and tool output messages, will be passed in here.\\n\\nLangChain has a lot of [predefined tool definitions to be reused](https://python.langchain.com/docs/integrations/tools/).\\n\\nWe can use tool calling in chain (to use tools in sequence) or in [agent](https://python.langchain.com/docs/modules/agents/agent_types/tool_calling/) (to use tools in loop).\\n\\nLangChain offers an API to the LLM called `bind_tools` to pass the definition of the tool, as part of each call to the model, so that the application can invoke the tool when appropriate.\\n\\nSee also [the load tools api with a list of predefined tools](https://api.python.langchain.com/en/latest/agents/langchain.agents.load_tools.load_tools.html#langchain.agents.load_tools.load_tools).\\n\\nBelow is the classical application flow using tool calling. The exposed function wraps a remote microservice.\\n\\nWhen developing a solution based on agent, consider the tools, the services, the agent needs to access. See a code example [openAI\\\\_agent.py](https://github.com/jbcodeforce/ML-studies/tree/master/llm-langchain/openAI/openAI_agent.py).\\n\\nMany LLM providers support for tool calling, including Anthropic, Cohere, Google, Mistral, OpenAI, see the [existing LangChain tools](https://python.langchain.com/docs/integrations/tools/).\\n\\n### Interesting tools[¶](#interesting-tools \"Permanent link\")\\n\\n#### Search recent news[¶](#search-recent-news \"Permanent link\")\\n\\nA common tool integrated in agent, is the [Tavily](https://tavily.com/) search API, used to get the last trusted News, so the most recent information created after the cutoff date of the LLM.\\n\\n```\\nretriever_tool = create_retriever_tool(retriever, \"langsmith_search\",\"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",) search = TavilySearchResults() tools =[retriever_tool, search]\\n```\\n\\n Tavily\\n\\n[Tavily](https://docs.tavily.com/) is the leading search engine optimized for LLMs. It provides factual, explicit and objective answers. It is a GPT researcher which queries, filters and aggregates over 20+ web sources per a single research task. It focuses on optimizing search for AI developers and autonomous AI agents. See [this git repo](https://github.com/assafelovic/gpt-researcher.git)\\n\\n#### Python REPLtool[¶](#python-repltool \"Permanent link\")\\n\\n[PythonREPLTool](https://api.python.langchain.com/en/latest/tools/langchain_experimental.tools.python.tool.PythonREPLTool.html#langchain_experimental.tools.python.tool.PythonREPLTool) is a tool for running python code in REPL (look like a jupiter notebook).\\n\\n#### A base model[¶](#a-base-model \"Permanent link\")\\n\\nIt is possible to bind a BaseModel class as below, where a LLM is used to create prompt, so the prompt instruction entity is a json used as tool. (Tool definition are structured system prompt for the LLMs as they just understand text)\\n\\n```\\nclass  PromptInstructions(BaseModel):  \"\"\"Instructions on how to prompt the LLM.\"\"\" objective: str variables: List[str] constraints: List[str] requirements: List[str] llm_with_tool = llm. bind_tools([PromptInstructions])\\n```\\n\\n[See the LangGraph sample: prompt\\\\_builder\\\\_graph.py](https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/langgraph/prompt_builder_graph.py).\\n\\n#### Our own tools[¶](#our-own-tools \"Permanent link\")\\n\\n[Define custom tool](https://python.langchain.com/docs/modules/tools/custom_tools/) using the `@tool` annotation on a function to expose it as a tool. It uses the function name as the tool name and the function’s docstring as the tool’s description.\\n\\nA second approach is to subclass the langchain.`pydantic_v1.BaseModel` class.\\n\\nFinally the last possible approach is to use `StructuredTool` dataclass.\\n\\nWhen doing agent we need to manage exception and implement handle\\\\_tool\\\\_error.\\n\\nTo map the tools to OpenAI function call there is a module called: `from langchain_core.utils.function_calling import convert_to_openai_function`.\\n\\nIt may be interesting to use embeddings to do tool selection before calling LLM. [See this code agent\\\\_wt\\\\_tool\\\\_retrieval.py](https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/openAI/agent_wt_tool_retrieval.py) The approach is to dynamically select the N tools we want at run time, without having to pass all the tool definitions within the context window. It uses a vector store to create embeddings for each tool description.\\n\\n### How Tos[¶](#how-tos \"Permanent link\")\\n\\n How to trace the agent execution?\\n\\n```\\nimport langchain langchain.debug = True \\n```\\n\\nOr use LangSmith   Defining an agent with tool calling, and the concept of scratchpad\\n\\nDefine an agent with 1/ a user input, 2/ a component for formatting intermediate steps (agent action, tool output pairs) (`format_to_openai_tool_messages`: convert (AgentAction, tool output) tuples into FunctionMessages), and 3/ a component for converting the output message into an agent action/agent finish:\\n\\n```\\n# x is the response from LLM agent =({\"input\": lambda x: x[\"input\"], \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(x[\"intermediate_steps\"]), \"chat_history\": lambda x: x[\"chat_history\"],} | prompt | llm_with_tools | OpenAIToolsAgentOutputParser())\\n```\\n\\n[OpenAIToolsAgentOutputParser](https://api.python.langchain.com/en/latest/_modules/langchain/agents/output_parsers/openai_tools.html) used with OpenAI models, as it relies on the specific tool\\\\_calls parameter from OpenAI to convey what tools to use.\\n\\n  How to support streaming the LLM\\'s output?\\n\\n[LangChain streaming](https://python.langchain.com/docs/expression_language/streaming/)  is needed to make the app more responsive for end-users. All [Runnable objects](https://python.langchain.com/v0.1/docs/expression_language/interface/) implement a sync method called `stream` and an `async` variant called `astream`. They cut output into chunks and yield them. Recall yield is a generator of data and acts as `return`. The main demo code is [web\\\\_server\\\\_wt\\\\_streaming](https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/openAI/web_server_wt_streaming.py) with the client\\\\_stream.py\\n\\n  Example of Intended Model\\n\\nto be done\\n\\n  Example of Supports Multi-Input Tools\\n\\nto be done\\n\\n  Use a vector store to keep the list of agent and description\\n\\nAs we cannot put the description of all the tools in the prompt (because of context length issues) so instead we dynamically select the N tools we do want to consider using, at run time. See the code in [agent\\\\_wt\\\\_tool\\\\_retrieval.py](https://github.com/jbcodeforce/ML-studies/blob/master/llm-langchain/openAI/agent_wt_tool_retrieval.py).\\n\\n## [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language)[¶](#langchain-expression-language-lcel \"Permanent link\")\\n\\nLCEL supports streaming the LLM results, use async communication, run in parallel, retries and fallbacks, access intermediate results.\\n\\n '},\n",
       " {'title': '2. Add tools - GitHub Pages',\n",
       "  'url': 'https://langchain-ai.github.io/langgraph/tutorials/get-started/2-add-tools/',\n",
       "  'content': 'Remember to pay attention to state management, conditional edges, and ensuring there are no dead-end nodes in your graph.\\', \\'score\\': 0.7065353, \\'raw_content\\': None}, {\\'title\\': \\'LangGraph Tutorial: What Is LangGraph and How to Use It?\\', \\'url\\': \\'https://www.datacamp.com/tutorial/langgraph-tutorial\\', \\'content\\': \\'LangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner. Let me do that for you.\", \\'type\\': \\'text\\'}, {\\'id\\': \\'toolu_01Q588CszHaSvvP2MxRq9zRD\\', \\'input\\': {\\'query\\': \\'LangGraph AI tool information\\'}, \\'name\\': \\'tavily_search_results_json\\', \\'type\\': \\'tool_use\\'}] Assistant: [{\"url\": \"https://www.langchain.com/langgraph\", \"content\": \"LangGraph sets the foundation for how we can build and scale AI workloads \\\\u2014 from conversational agents, complex task automation, to custom LLM-backed experiences that \\'just work\\'.',\n",
       "  'score': 0.97233,\n",
       "  'raw_content': '[Skip to content](#add-tools)\\n\\n\\n\\n* [Prerequisites](#prerequisites)\\n* [1. Install the search engine](#1-install-the-search-engine)\\n* [2. Configure your environment](#2-configure-your-environment)\\n* [3. Define the tool](#3-define-the-tool)\\n* [4. Define the graph](#4-define-the-graph)\\n* [5. Create a function to run the tools](#5-create-a-function-to-run-the-tools)\\n* [6. Define the conditional\\\\_edges](#6-define-the-conditional_edges)\\n* [7. Visualize the graph (optional)](#7-visualize-the-graph-optional)\\n* [8. Ask the bot questions](#8-ask-the-bot-questions)\\n* [9. Use prebuilts](#9-use-prebuilts)\\n* [Next steps](#next-steps)\\n\\n# Add tools[¶](#add-tools \"Permanent link\")\\n\\nTo handle queries that your chatbot can\\'t answer \"from memory\", integrate a web search tool. The chatbot can use this tool to find relevant information and provide better responses.\\n\\nNote\\n\\nThis tutorial builds on [Build a basic chatbot](../1-build-basic-chatbot/).\\n\\n## Prerequisites[¶](#prerequisites \"Permanent link\")\\n\\nBefore you start this tutorial, ensure you have the following:\\n\\n* An API key for the [Tavily Search Engine](https://python.langchain.com/docs/integrations/tools/tavily_search/).\\n\\n## 1. Install the search engine[¶](#1-install-the-search-engine \"Permanent link\")\\n\\nInstall the requirements to use the [Tavily Search Engine](https://python.langchain.com/docs/integrations/tools/tavily_search/):\\n\\n```\\npip install -U langchain-tavily    \\n```\\n\\n## 2. Configure your environment[¶](#2-configure-your-environment \"Permanent link\")\\n\\nConfigure your environment with your search engine API key:\\n\\n```\\nimport os import  os  os.environ[\"TAVILY_API_KEY\"] = \"tvly-...\" os. environ[\"TAVILY_API_KEY\"] =\"tvly-...\"\\n```\\n\\n## 3. Define the tool[¶](#3-define-the-tool \"Permanent link\")\\n\\nDefine the web search tool:\\n\\n*API Reference: [TavilySearch](https://python.langchain.com/api_reference/tavily/tavily_search/langchain_tavily.tavily_search.TavilySearch.html)*\\n\\n```\\nfrom langchain_tavily import TavilySearch from  langchain_tavily  import TavilySearch  tool = TavilySearch(max_results=2) tool = TavilySearch(max_results = 2)tools = [tool] tools =[tool]tool.invoke(\"What\\'s a \\'node\\' in LangGraph?\") tool. invoke(\"What\\'s a \\'node\\' in LangGraph?\")\\n```\\n\\nThe results are page summaries our chat bot can use to answer questions:\\n\\n```\\n{\\'query\\': \"What\\'s a \\'node\\' in LangGraph?\", \\'follow_up_questions\\': None, \\'answer\\': None, \\'images\\': [], \\'results\\': [{\\'title\\': \"Introduction to LangGraph: A Beginner\\'s Guide - Medium\", \\'url\\': \\'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141\\', \\'content\\': \\'Stateful Graph: LangGraph revolves around the concept of a stateful graph, where each node in the graph represents a step in your computation, and the graph maintains a state that is passed around and updated as the computation progresses. LangGraph supports conditional edges, allowing you to dynamically determine the next node to execute based on the current state of the graph. We define nodes for classifying the input, handling greetings, and handling search queries. def classify_input_node(state): LangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its power for their projects. Remember to pay attention to state management, conditional edges, and ensuring there are no dead-end nodes in your graph.\\', \\'score\\': 0.7065353, \\'raw_content\\': None}, {\\'title\\': \\'LangGraph Tutorial: What Is LangGraph and How to Use It?\\', \\'url\\': \\'https://www.datacamp.com/tutorial/langgraph-tutorial\\', \\'content\\': \\'LangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner. By managing the flow of data and the sequence of operations, LangGraph allows developers to focus on the high-level logic of their applications rather than the intricacies of agent coordination. Whether you need a chatbot that can handle various types of user requests or a multi-agent system that performs complex tasks, LangGraph provides the tools to build exactly what you need. LangGraph significantly simplifies the development of complex LLM applications by providing a structured framework for managing state and coordinating agent interactions.\\', \\'score\\': 0.5008063, \\'raw_content\\': None}], \\'response_time\\': 1.38} \\n```\\n\\n## 4. Define the graph[¶](#4-define-the-graph \"Permanent link\")\\n\\nFor the `StateGraph` you created in the [first tutorial](../1-build-basic-chatbot/), add `bind_tools` on the LLM. This lets the LLM know the correct JSON format to use if it wants to use the search engine.\\n\\nLet\\'s first select our LLM:\\n\\n```\\npip install -U \"langchain[openai]\"    \"langchain[openai]\"\\n```\\n\\n```\\nimport os import  osfrom langchain.chat_models import init_chat_model from  langchain.chat_models  import init_chat_model  os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" os. environ[\"OPENAI_API_KEY\"] =\"sk-...\"  llm = init_chat_model(\"openai:gpt-4.1\") llm = init_chat_model(\"openai:gpt-4.1\")\\n```\\n\\n👉 Read the [OpenAI integration docs](https://python.langchain.com/docs/integrations/chat/openai/)\\n\\n```\\npip install -U \"langchain[anthropic]\"    \"langchain[anthropic]\"\\n```\\n\\n```\\nimport os import  osfrom langchain.chat_models import init_chat_model from  langchain.chat_models  import init_chat_model  os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\" os. environ[\"ANTHROPIC_API_KEY\"] =\"sk-...\"  llm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\") llm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\\n```\\n\\n👉 Read the [Anthropic integration docs](https://python.langchain.com/docs/integrations/chat/anthropic/)\\n\\n```\\npip install -U \"langchain[openai]\"    \"langchain[openai]\"\\n```\\n\\n```\\nimport os import  osfrom langchain.chat_models import init_chat_model from  langchain.chat_models  import init_chat_model  os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\" os. environ[\"AZURE_OPENAI_API_KEY\"] =\"...\"os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\" os. environ[\"AZURE_OPENAI_ENDPOINT\"] =\"...\"os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\" os. environ[\"OPENAI_API_VERSION\"] =\"2025-03-01-preview\"  llm = init_chat_model( llm = init_chat_model( \"azure_openai:gpt-4.1\", \"azure_openai:gpt-4.1\", azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"], azure_deployment = os. environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],) )\\n```\\n\\n👉 Read the [Azure integration docs](https://python.langchain.com/docs/integrations/chat/azure_chat_openai/)\\n\\n```\\npip install -U \"langchain[google-genai]\"    \"langchain[google-genai]\"\\n```\\n\\n```\\nimport os import  osfrom langchain.chat_models import init_chat_model from  langchain.chat_models  import init_chat_model  os.environ[\"GOOGLE_API_KEY\"] = \"...\" os. environ[\"GOOGLE_API_KEY\"] =\"...\"  llm = init_chat_model(\"google_genai:gemini-2.0-flash\") llm = init_chat_model(\"google_genai:gemini-2.0-flash\")\\n```\\n\\n👉 Read the [Google GenAI integration docs](https://python.langchain.com/docs/integrations/chat/google_generative_ai/)\\n\\n```\\npip install -U \"langchain[aws]\"    \"langchain[aws]\"\\n```\\n\\n```\\nfrom langchain.chat_models import init_chat_model from  langchain.chat_models  import init_chat_model  # Follow the steps here to configure your credentials: # Follow the steps here to configure your credentials:# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html  llm = init_chat_model( llm = init_chat_model( \"anthropic.claude-3-5-sonnet-20240620-v1:0\", \"anthropic.claude-3-5-sonnet-20240620-v1:0\", model_provider=\"bedrock_converse\", model_provider = \"bedrock_converse\",) )\\n```\\n\\n👉 Read the [AWS Bedrock integration docs](https://python.langchain.com/docs/integrations/chat/bedrock/)\\n\\nWe can now incorporate it into a `StateGraph`:\\n\\n*API Reference: [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [START](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) | [END](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.END) | [add\\\\_messages](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages)*\\n\\n```\\nfrom typing import Annotated from  typing  import Annotated  from typing_extensions import TypedDict from  typing_extensions  import TypedDict  from langgraph.graph import StateGraph, START, END from  langgraph.graph  import StateGraph, START, ENDfrom langgraph.graph.message import add_messages from  langgraph.graph.message  import add_messages  class State(TypedDict): class  State(TypedDict): messages: Annotated[list, add_messages] messages: Annotated[list, add_messages]  graph_builder = StateGraph(State) graph_builder = StateGraph(State)  # Modification: tell the LLM which tools it can call # Modification: tell the LLM which tools it can callllm_with_tools = llm.bind_tools(tools) llm_with_tools = llm.bind_tools(tools) llm_with_tools = llm. bind_tools(tools)  def chatbot(state: State): def  chatbot(state: State): return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]} return{\"messages\":[llm_with_tools. invoke(state[\"messages\"])]}  graph_builder.add_node(\"chatbot\", chatbot) graph_builder. add_node(\"chatbot\", chatbot)\\n```\\n\\n## 5. Create a function to run the tools[¶](#5-create-a-function-to-run-the-tools \"Permanent link\")\\n\\nNow, create a function to run the tools if they are called. Do this by adding the tools to a new node called `BasicToolNode` that checks the most recent message in the state and calls tools if the message contains `tool_calls`. It relies on the LLM\\'s `tool_calling` support, which is available in Anthropic, OpenAI, Google Gemini, and a number of other LLM providers.\\n\\n*API Reference: [ToolMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html)*\\n\\n```\\nimport json import  json  from langchain_core.messages import ToolMessage from  langchain_core.messages  import ToolMessage   class BasicToolNode: class  BasicToolNode: \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"  \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"   def __init__(self, tools: list) -> None: def  __init__(self, tools: list) -> None: self.tools_by_name = {tool.name: tool for tool in tools} self. tools_by_name ={tool. name: tool for tool in tools}   def __call__(self, inputs: dict): def  __call__(self, inputs: dict): if messages := inputs.get(\"messages\", []): if messages:= inputs. get(\"messages\",[]): message = messages[-1] message = messages[- 1] else: else: raise ValueError(\"No message found in input\") raise ValueError(\"No message found in input\") outputs = [] outputs =[] for tool_call in message.tool_calls: for tool_call in message. tool_calls: tool_result = self.tools_by_name[tool_call[\"name\"]].invoke( tool_result = self. tools_by_name[tool_call[\"name\"]]. invoke( tool_call[\"args\"] tool_call[\"args\"] ) ) outputs.append( outputs. append( ToolMessage( ToolMessage( content=json.dumps(tool_result), content = json. dumps(tool_result), name=tool_call[\"name\"], name = tool_call[\"name\"], tool_call_id=tool_call[\"id\"], tool_call_id = tool_call[\"id\"], ) ) ) ) return {\"messages\": outputs} return{\"messages\": outputs}   tool_node = BasicToolNode(tools=[tool]) tool_node = BasicToolNode(tools =[tool])graph_builder.add_node(\"tools\", tool_node) graph_builder. add_node(\"tools\", tool_node)\\n```\\n\\nNote\\n\\nIf you do not want to build this yourself in the future, you can use LangGraph\\'s prebuilt [ToolNode](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.tool_node.ToolNode).\\n\\n## 6. Define the `conditional_edges`[¶](#6-define-the-conditional_edges \"Permanent link\")\\n\\nWith the tool node added, now you can define the `conditional_edges`.\\n\\n**Edges** route the control flow from one node to the next. **Conditional edges** start from a single node and usually contain \"if\" statements to route to different nodes depending on the current graph state. These functions receive the current graph `state` and return a string or list of strings indicating which node(s) to call next.\\n\\nNext, define a router function called `route_tools` that checks for `tool_calls` in the chatbot\\'s output. Provide this function to the graph by calling `add_conditional_edges`, which tells the graph that whenever the `chatbot` node completes to check this function to see where to go next.\\n\\nThe condition will route to `tools` if tool calls are present and `END` if not. Because the condition can return `END`, you do not need to explicitly set a `finish_point` this time.\\n\\n```\\ndef route_tools( def  route_tools( state: State, state: State,): ):  \"\"\"  \"\"\"  Use in the conditional_edge to route to the ToolNode if the last message  Use in the conditional_edge to route to the ToolNode if the last message has tool calls. Otherwise, route to the end.  has tool calls. Otherwise, route to the end.  \"\"\"  \"\"\" if isinstance(state, list): if isinstance(state, list): ai_message = state[-1] ai_message = state[- 1] elif messages := state.get(\"messages\", []): elif messages:= state. get(\"messages\",[]): ai_message = messages[-1] ai_message = messages[- 1] else: else: raise ValueError(f\"No messages found in input state to tool_edge: {state}\") raise ValueError(f\"No messages found in input state to tool_edge: {state} \") if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0: if hasattr(ai_message, \"tool_calls\") and len(ai_message. tool_calls)> 0:  return \"tools\" return \"tools\"  return END return END   # The `tools_condition` function returns \"tools\" if the chatbot asks to use a tool, and \"END\" if # The `tools_condition` function returns \"tools\" if the chatbot asks to use a tool, and \"END\" if# it is fine directly responding. This conditional routing defines the main agent loop. # it is fine directly responding. This conditional routing defines the main agent loop.graph_builder.add_conditional_edges( graph_builder. add_conditional_edges( \"chatbot\", \"chatbot\", route_tools, route_tools,  # The following dictionary lets you tell the graph to interpret the condition\\'s outputs as a specific node # The following dictionary lets you tell the graph to interpret the condition\\'s outputs as a specific node # It defaults to the identity function, but if you # It defaults to the identity function, but if you # want to use a node named something else apart from \"tools\", # want to use a node named something else apart from \"tools\",  # You can update the value of the dictionary to something else # You can update the value of the dictionary to something else # e.g., \"tools\": \"my_tools\" # e.g., \"tools\": \"my_tools\" {\"tools\": \"tools\", END: END}, {\"tools\": \"tools\", END: END},) )# Any time a tool is called, we return to the chatbot to decide the next step # Any time a tool is called, we return to the chatbot to decide the next stepgraph_builder.add_edge(\"tools\", \"chatbot\") graph_builder. add_edge(\"tools\", \"chatbot\")graph_builder.add_edge(START, \"chatbot\") graph_builder. add_edge(START, \"chatbot\")graph = graph_builder.compile() graph = graph_builder. compile()\\n```\\n\\nNote\\n\\nYou can replace this with the prebuilt [tools\\\\_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#tools_condition) to be more concise.\\n\\n## 7. Visualize the graph (optional)[¶](#7-visualize-the-graph-optional \"Permanent link\")\\n\\nYou can visualize the graph using the `get_graph` method and one of the \"draw\" methods, like `draw_ascii` or `draw_png`. The `draw` methods each require additional dependencies.\\n\\n```\\nfrom IPython.display import Image, display from  IPython.display  import Image, display  try: try: display(Image(graph.get_graph().draw_mermaid_png())) display(Image(graph. get_graph(). draw_mermaid_png()))except Exception: except Exception:  # This requires some extra dependencies and is optional # This requires some extra dependencies and is optional  pass pass\\n```\\n\\n## 8. Ask the bot questions[¶](#8-ask-the-bot-questions \"Permanent link\")\\n\\nNow you can ask the chatbot questions outside its training data:\\n\\n```\\ndef stream_graph_updates(user_input: str): def  stream_graph_updates(user_input: str): for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}): for event in graph. stream({\"messages\":[{\"role\": \"user\", \"content\": user_input}]}): for value in event.values(): for value in event. values(): print(\"Assistant:\", value[\"messages\"][-1].content) print(\"Assistant:\", value[\"messages\"][- 1]. content)  while True: while True: try: try: user_input = input(\"User: \") user_input = input(\"User: \") if user_input.lower() in [\"quit\", \"exit\", \"q\"]: if user_input. lower() in[\"quit\", \"exit\", \"q\"]: print(\"Goodbye!\") print(\"Goodbye!\")  break break   stream_graph_updates(user_input) stream_graph_updates(user_input) except: except: # fallback if input() is not available # fallback if input() is not available user_input = \"What do you know about LangGraph?\" user_input = \"What do you know about LangGraph?\" print(\"User: \" + user_input) print(\"User: \" + user_input) stream_graph_updates(user_input) stream_graph_updates(user_input)  break break\\n```\\n\\n```\\nAssistant: [{\\'text\\': \"To provide you with accurate and up-to-date information about LangGraph, I\\'ll need to search for the latest details. Let me do that for you.\", \\'type\\': \\'text\\'}, {\\'id\\': \\'toolu_01Q588CszHaSvvP2MxRq9zRD\\', \\'input\\': {\\'query\\': \\'LangGraph AI tool information\\'}, \\'name\\': \\'tavily_search_results_json\\', \\'type\\': \\'tool_use\\'}] Assistant: [{\"url\": \"https://www.langchain.com/langgraph\", \"content\": \"LangGraph sets the foundation for how we can build and scale AI workloads \\\\u2014 from conversational agents, complex task automation, to custom LLM-backed experiences that \\'just work\\'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution ...\"}, {\"url\": \"https://github.com/langchain-ai/langgraph\", \"content\": \"Overview. LangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures ...\"}] Assistant: Based on the search results, I can provide you with information about LangGraph:  1. Purpose:  LangGraph is a library designed for building stateful, multi-actor applications with Large Language Models (LLMs). It\\'s particularly useful for creating agent and multi-agent workflows.  2. Developer:  LangGraph is developed by LangChain, a company known for its tools and frameworks in the AI and LLM space.  3. Key Features:  - Cycles: LangGraph allows the definition of flows that involve cycles, which is essential for most agentic architectures.  - Controllability: It offers enhanced control over the application flow.  - Persistence: The library provides ways to maintain state and persistence in LLM-based applications.  4. Use Cases:  LangGraph can be used for various applications, including:  - Conversational agents  - Complex task automation  - Custom LLM-backed experiences  5. Integration:  LangGraph works in conjunction with LangSmith, another tool by LangChain, to provide an out-of-the-box solution for building complex, production-ready features with LLMs.  6. Significance: ...  LangGraph is noted to offer unique benefits compared to other LLM frameworks, particularly in its ability to handle cycles, provide controllability, and maintain persistence.  LangGraph appears to be a significant tool in the evolving landscape of LLM-based application development, offering developers new ways to create more complex, stateful, and interactive AI systems. Goodbye! \\n```\\n\\n## 9. Use prebuilts[¶](#9-use-prebuilts \"Permanent link\")\\n\\nFor ease of use, adjust your code to replace the following with LangGraph prebuilt components. These have built in functionality like parallel API execution.\\n\\n* `BasicToolNode` is replaced with the prebuilt [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#toolnode)\\n* `route_tools` is replaced with the prebuilt [tools\\\\_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#tools_condition)\\n\\n```\\npip install -U \"langchain[openai]\"    \"langchain[openai]\"\\n```\\n\\n```\\nimport os import  osfrom langchain.chat_models import init_chat_model from  langchain.chat_models  import init_chat_model  os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" os. environ[\"OPENAI_API_KEY\"] =\"sk-...\"  llm = init_chat_model(\"openai:gpt-4.1\") llm = init_chat_model(\"openai:gpt-4.1\")\\n```\\n\\n👉 Read the [OpenAI integration docs](https://python.langchain.com/docs/integrations/chat/openai/)\\n\\n```\\npip install -U \"langchain[anthropic]\"    \"langchain[anthropic]\"\\n```\\n\\n```\\nimport os import  osfrom langchain.chat_models import init_chat_model from  langchain.chat_models  import init_chat_model  os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\" os. environ[\"ANTHROPIC_API_KEY\"] =\"sk-...\"  llm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\") llm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\\n```\\n\\n👉 Read the [Anthropic integration docs](https://python.langchain.com/docs/integrations/chat/anthropic/)\\n\\n```\\npip install -U \"langchain[openai]\"    \"langchain[openai]\"\\n```\\n\\n```\\nimport os import  osfrom langchain.chat_models import init_chat_model from  langchain.chat_models  import init_chat_model  os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\" os. environ[\"AZURE_OPENAI_API_KEY\"] =\"...\"os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\" os. environ[\"AZURE_OPENAI_ENDPOINT\"] =\"...\"os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\" os. environ[\"OPENAI_API_VERSION\"] =\"2025-03-01-preview\"  llm = init_chat_model( llm = init_chat_model( \"azure_openai:gpt-4.1\", \"azure_openai:gpt-4.1\", azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"], azure_deployment = os. environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],) )\\n```\\n\\n👉 Read the [Azure integration docs](https://python.langchain.com/docs/integrations/chat/azure_chat_openai/)\\n\\n```\\npip install -U \"langchain[google-genai]\"    \"langchain[google-genai]\"\\n```\\n\\n```\\nimport os import  osfrom langchain.chat_models import init_chat_model from  langchain.chat_models  import init_chat_model  os.environ[\"GOOGLE_API_KEY\"] = \"...\" os. environ[\"GOOGLE_API_KEY\"] =\"...\"  llm = init_chat_model(\"google_genai:gemini-2.0-flash\") llm = init_chat_model(\"google_genai:gemini-2.0-flash\")\\n```\\n\\n👉 Read the [Google GenAI integration docs](https://python.langchain.com/docs/integrations/chat/google_generative_ai/)\\n\\n```\\npip install -U \"langchain[aws]\"    \"langchain[aws]\"\\n```\\n\\n```\\nfrom langchain.chat_models import init_chat_model from  langchain.chat_models  import init_chat_model  # Follow the steps here to configure your credentials: # Follow the steps here to configure your credentials:# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html  llm = init_chat_model( llm = init_chat_model( \"anthropic.claude-3-5-sonnet-20240620-v1:0\", \"anthropic.claude-3-5-sonnet-20240620-v1:0\", model_provider=\"bedrock_converse\", model_provider = \"bedrock_converse\",) )\\n```\\n\\n👉 Read the [AWS Bedrock integration docs](https://python.langchain.com/docs/integrations/chat/bedrock/)\\n\\n```\\nfrom typing import Annotated from  typing  import Annotated  from langchain_tavily import TavilySearch from  langchain_tavily  import TavilySearchfrom langchain_core.messages import BaseMessage from  langchain_core.messages  import BaseMessage from typing_extensions import TypedDict from  typing_extensions  import TypedDict  from langgraph.graph import StateGraph, START, END from  langgraph.graph  import StateGraph, START, ENDfrom langgraph.graph.message import add_messages from  langgraph.graph.message  import add_messagesfrom langgraph.prebuilt import ToolNode, tools_condition from  langgraph.prebuilt  import ToolNode, tools_condition  class State(TypedDict): class  State(TypedDict): messages: Annotated[list, add_messages] messages: Annotated[list, add_messages]  graph_builder = StateGraph(State) graph_builder = StateGraph(State)  tool = TavilySearch(max_results=2) tool = TavilySearch(max_results = 2)tools = [tool] tools =[tool]llm_with_tools = llm.bind_tools(tools) llm_with_tools = llm. bind_tools(tools)  def chatbot(state: State): def  chatbot(state: State): return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]} return{\"messages\":[llm_with_tools. invoke(state[\"messages\"])]}  graph_builder.add_node(\"chatbot\", chatbot) graph_builder. add_node(\"chatbot\", chatbot)  tool_node = ToolNode(tools=[tool]) tool_node = ToolNode(tools=[tool]) tool_node = ToolNode(tools =[tool])graph_builder.add_node(\"tools\", tool_node) graph_builder. add_node(\"tools\", tool_node)  graph_builder.add_conditional_edges( graph_builder. add_conditional_edges( \"chatbot\", \"chatbot\", tools_condition,  tools_condition, tools_condition,) )# Any time a tool is called, we return to the chatbot to decide the next step # Any time a tool is called, we return to the chatbot to decide the next stepgraph_builder.add_edge(\"tools\", \"chatbot\") graph_builder. add_edge(\"tools\", \"chatbot\")graph_builder.add_edge(START, \"chatbot\") graph_builder. add_edge(START, \"chatbot\")graph = graph_builder.compile() graph = graph_builder. compile()\\n```\\n\\n**Congratulations!** You\\'ve created a conversational agent in LangGraph that can use a search engine to retrieve updated information when needed. Now it can handle a wider range of user queries.\\n\\nTo inspect all the steps your agent just took, check out this [LangSmith trace](https://smith.langchain.com/public/4fbd7636-25af-4638-9587-5a02fdbb0172/r).\\n\\n## Next steps[¶](#next-steps \"Permanent link\")\\n\\nThe chatbot cannot remember past interactions on its own, which limits its ability to have coherent, multi-turn conversations. In the next part, you will [add **memory**](../3-add-memory/) to address this.\\n\\n '}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.invoke({\n",
    "    \"query\":\"LangChain Tool에 대해서 알려주세요\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Tools \n",
    "\n",
    "- LangChain에서 제공하는 빌트인 도구와 별도로, 사용자가 직접 도구를 정의하교 사용 \n",
    "- 이를 위해 `langchain.tools` 모듈에서 제공하는 `tool` decorator를 사용하여 파이썬 함수를 도구로 변환 \n",
    "\n",
    "### @tool decorator \n",
    "- 파이썬 함수를 도구로 변환 하는 기능 \n",
    "- 사용 방법 \n",
    "    - 함수 위에 `@tool` 데코레이터 적용 \n",
    "    - 필요에 따라 데코레이터 매개변수 설정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데코레이터 추가하여 함수 -> 도구로 변환 \n",
    "# LLM이 함수를 호출할 때 함수의 역할을 확인할 수 있도록 주석을 반드시 추가 (in English)\n",
    "\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\" Add two numbers\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def mult_numbers(a: int, b: int) -> int:\n",
    "    \"\"\" Multiply two numbers\n",
    "    \"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_numbers.invoke({'a': 10, 'b':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mult_numbers.invoke({'a': 10, 'b':5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "** End of Documents **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
